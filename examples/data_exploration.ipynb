{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "import wilds\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.datasets.unlabeled.wilds_unlabeled_dataset import WILDSPseudolabeledSubset\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool, get_model_prefix, move_to\n",
    "from train import train, evaluate, infer_predictions\n",
    "from algorithms.initializer import initialize_algorithm, infer_d_out\n",
    "from transforms import initialize_transform\n",
    "from models.initializer import initialize_model\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported\n",
    "\n",
    "import torch.multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required arguments\n",
    "\n",
    "args = [\n",
    "    '--dataset', 'fmow',\n",
    "    '--algorithm', 'ERM',\n",
    "    '--root_dir', 'data',\n",
    "    '--model', 'vit_b_16',\n",
    "    '--progress_bar'\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-d', '--dataset', choices=wilds.supported_datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                  help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for dataset initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                  help='If true, tries to download the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                  help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes. Note that this also scales the test set down, so the reported numbers are not comparable with the full test set.')\n",
    "parser.add_argument('--version', default=None, type=str, help='WILDS labeled dataset version number.')\n",
    "\n",
    "# Unlabeled Dataset\n",
    "parser.add_argument('--unlabeled_split', default=None, type=str, choices=wilds.unlabeled_splits,  help='Unlabeled split to use. Some datasets only have some splits available.')\n",
    "parser.add_argument('--unlabeled_version', default=None, type=str, help='WILDS unlabeled dataset version number.')\n",
    "parser.add_argument('--use_unlabeled_y', default=False, type=parse_bool, const=True, nargs='?', \n",
    "                  help='If true, unlabeled loaders will also the true labels for the unlabeled data. This is only available for some datasets. Used for \"fully-labeled ERM experiments\" in the paper. Correct functionality relies on CrossEntropyLoss using ignore_index=-100.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--unlabeled_loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?', help='If true, sample examples such that batches are uniform over groups.')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?', help='If true, enforce groups sampled per batch are distinct.')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--unlabeled_n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--unlabeled_batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of batches to process before stepping optimizer and schedulers. If > 1, we simulate having a larger effective batch size (though batchnorm behaves differently).')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--noisystudent_add_dropout', type=parse_bool, const=True, nargs='?', help='If true, adds a dropout layer to the student model of NoisyStudent.')\n",
    "parser.add_argument('--noisystudent_dropout_rate', type=float)\n",
    "parser.add_argument('--pretrained_model_path', default=None, type=str, help='Specify a path to pretrained model weights')\n",
    "parser.add_argument('--load_featurizer_only', default=False, type=parse_bool, const=True, nargs='?', help='If true, only loads the featurizer weights and not the classifier weights.')\n",
    "\n",
    "# NoisyStudent-specific loading\n",
    "parser.add_argument('--teacher_model_path', type=str, help='Path to NoisyStudent teacher model weights. If this is defined, pseudolabels will first be computed for unlabeled data before anything else runs.')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--transform', choices=supported.transforms)\n",
    "parser.add_argument('--additional_train_transform', choices=supported.additional_transforms, help='Optional data augmentations to layer on top of the default transforms.')\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='The input resolution that images will be resized to before being passed into the model. For example, use --target_resolution 224 224 for a standard ResNet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "parser.add_argument('--randaugment_n', type=int, help='Number of RandAugment transformations to apply.')\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices=supported.losses)\n",
    "parser.add_argument('--loss_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for loss initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--dann_penalty_weight', type=float)\n",
    "parser.add_argument('--dann_classifier_lr', type=float)\n",
    "parser.add_argument('--dann_featurizer_lr', type=float)\n",
    "parser.add_argument('--dann_discriminator_lr', type=float)\n",
    "parser.add_argument('--afn_penalty_weight', type=float)\n",
    "parser.add_argument('--safn_delta_r', type=float)\n",
    "parser.add_argument('--hafn_r', type=float)\n",
    "parser.add_argument('--use_hafn', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--self_training_lambda', type=float)\n",
    "parser.add_argument('--self_training_threshold', type=float)\n",
    "parser.add_argument('--pseudolabel_T2', type=float, help='Percentage of total iterations at which to end linear scheduling and hold lambda at the max value')\n",
    "parser.add_argument('--soft_pseudolabels', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--algo_log_metric')\n",
    "parser.add_argument('--process_pseudolabels_function', choices=supported.process_pseudolabels_functions)\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for optimizer initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for scheduler initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--process_outputs_function', choices = supported.process_outputs_functions)\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int, help='If eval_only is set, then eval_epoch allows you to specify evaluating at a particular epoch. By default, it evaluates the best epoch by validation performance.')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, nargs='+', default=[0])\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_pred', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False, help='Whether to resume from the most recent saved model in the current log_dir.')\n",
    "\n",
    "# Weights & Biases\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--wandb_api_key_path', type=str,\n",
    "                  help=\"Path to Weights & Biases API Key. If use_wandb is set to True and this argument is not specified, user will be prompted to authenticate.\")\n",
    "parser.add_argument('--wandb_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                  help='keyword arguments for wandb.init() passed as key1=value1 key2=value2')\n",
    "\n",
    "config = parser.parse_args(args)\n",
    "config = populate_defaults(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: fmow\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {'seed': 111, 'use_ood_val': True}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Version: None\n",
      "Unlabeled split: None\n",
      "Unlabeled version: None\n",
      "Use unlabeled y: False\n",
      "Loader kwargs: {'num_workers': 4, 'pin_memory': True}\n",
      "Unlabeled loader kwargs: {'num_workers': 8, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 8\n",
      "Unlabeled n groups per batch: 8\n",
      "Batch size: 32\n",
      "Unlabeled batch size: 32\n",
      "Eval loader: standard\n",
      "Gradient accumulation steps: 1\n",
      "Model: densenet121\n",
      "Model kwargs: {'pretrained': True}\n",
      "Noisystudent add dropout: None\n",
      "Noisystudent dropout rate: None\n",
      "Pretrained model path: None\n",
      "Load featurizer only: False\n",
      "Teacher model path: None\n",
      "Transform: image_base\n",
      "Additional train transform: None\n",
      "Target resolution: (224, 224)\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Randaugment n: 2\n",
      "Loss function: cross_entropy\n",
      "Loss kwargs: {}\n",
      "Groupby fields: ['year']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: 0.1\n",
      "Dann penalty weight: 1.0\n",
      "Dann classifier lr: 0.0001\n",
      "Dann featurizer lr: 1e-05\n",
      "Dann discriminator lr: 0.0001\n",
      "Afn penalty weight: None\n",
      "Safn delta r: None\n",
      "Hafn r: None\n",
      "Use hafn: False\n",
      "Irm lambda: 1.0\n",
      "Irm penalty anneal iters: None\n",
      "Self training lambda: None\n",
      "Self training threshold: None\n",
      "Pseudolabel t2: None\n",
      "Soft pseudolabels: False\n",
      "Algo log metric: accuracy\n",
      "Process pseudolabels function: pseudolabel_multiclass_logits\n",
      "Val metric: acc_worst_region\n",
      "Val metric decreasing: False\n",
      "N epochs: 60\n",
      "Optimizer: Adam\n",
      "Lr: 0.0001\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Optimizer kwargs: {}\n",
      "Scheduler: StepLR\n",
      "Scheduler kwargs: {'gamma': 0.96, 'step_size': 1}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Process outputs function: multiclass_logits_to_pred\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: [0]\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "Save pred: True\n",
      "No group logging: False\n",
      "Progress bar: True\n",
      "Resume: False\n",
      "Use wandb: False\n",
      "Wandb api key path: None\n",
      "Wandb kwargs: {}\n",
      "Usable metadata len: 2\n",
      "\n",
      "['region', 'year', 'y', 'from_source_domain']\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "['Asia', 'Europe', 'Africa', 'Americas', 'Oceania', 'Other']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../predictions/baseline (fmow, 10)/fmow_seed:1_epoch:best_model.pth\n",
      "evaluating\n",
      "test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691/691 [01:09<00:00,  9.96it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m     is_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneral_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_best\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# for labeled_batch in train_loader:\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#     x, y, metadata = labeled_batch\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m    188\u001b[0m logger\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/wilds/examples/train.py:180\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(algorithm, datasets, epoch, general_logger, config, is_best)\u001b[0m\n\u001b[1;32m    178\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_y_true.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, save_y_true)\n\u001b[1;32m    179\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_y_true.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, save_y_true)\n\u001b[0;32m--> 180\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    182\u001b[0m results, results_str \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39meval(\n\u001b[1;32m    183\u001b[0m     epoch_y_pred,\n\u001b[1;32m    184\u001b[0m     epoch_y_true,\n\u001b[1;32m    185\u001b[0m     epoch_metadata)\n\u001b[1;32m    187\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m epoch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = 'fmow'\n",
    "root_dir = '../data'\n",
    "split_scheme = 'official'\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Initialize logs\n",
    "if os.path.exists(config.log_dir) and config.resume:\n",
    "    resume=True\n",
    "    mode='a'\n",
    "elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "    resume=False\n",
    "    mode='a'\n",
    "else:\n",
    "    resume=False\n",
    "    mode='w'\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "\n",
    "# Record config\n",
    "log_config(config, logger)\n",
    "\n",
    "dataset_kwargs= {\n",
    "        'seed': 111,\n",
    "        'use_ood_val': True\n",
    "        }\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if len(config.device) > device_count:\n",
    "        raise ValueError(f\"Specified {len(config.device)} devices, but only {device_count} devices found.\")\n",
    "\n",
    "    config.use_data_parallel = len(config.device) > 1\n",
    "    device_str = \",\".join(map(str, config.device))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_str\n",
    "    config.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    config.use_data_parallel = False\n",
    "    config.device = torch.device(\"cpu\")\n",
    "\n",
    "full_dataset = wilds.get_dataset(\n",
    "        dataset=config.dataset,\n",
    "        version=config.version,\n",
    "        root_dir=root_dir,\n",
    "        download=config.download,\n",
    "        split_scheme=config.split_scheme,\n",
    "        **config.dataset_kwargs)\n",
    "\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    additional_transform_name=config.additional_train_transform,\n",
    "    is_training=True)\n",
    "\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    is_training=False)\n",
    "\n",
    "no_transform = transforms.Compose(\n",
    "        [ transforms.ToTensor()]\n",
    "    )\n",
    "train_grouper = CombinatorialGrouper(\n",
    "            dataset=full_dataset,\n",
    "            groupby_fields=config.groupby_fields\n",
    "        )\n",
    "datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "\n",
    "    # Loggers\n",
    "    datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n",
    "    datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print(full_dataset.metadata_fields)\n",
    "\n",
    "all_regions = list(full_dataset.metadata['region'].unique())\n",
    "print(all_regions)\n",
    "print(full_dataset._metadata_map['region'])\n",
    "\n",
    "\n",
    "train_grouper = CombinatorialGrouper(\n",
    "            dataset=full_dataset,\n",
    "            groupby_fields=['year']\n",
    "        )\n",
    "\n",
    "train_data = full_dataset.get_subset(\n",
    "    \"train\",\n",
    "    transform=no_transform,\n",
    ")\n",
    "\n",
    "train_loader = get_train_loader(\"standard\", train_data, batch_size=16)\n",
    "\n",
    "algorithm = initialize_algorithm(\n",
    "        config=config,\n",
    "        datasets=datasets,\n",
    "        train_grouper=train_grouper,\n",
    "    )\n",
    "\n",
    "model_prefix = get_model_prefix(datasets['train'], config)\n",
    "#eval_model_path = \"../predictions/baseline (fmow, 10)/\" + 'fmow_seed:1_' + 'epoch:best_model.pth'\n",
    "eval_model_path = \"../predictions/V1.1 (fmow, 10)\" + 'fmow_seed:0_' + 'epoch:best_model.pth'\n",
    "print(eval_model_path)\n",
    "#load(algorithm, eval_model_path, device=config.device)\n",
    "\n",
    "# need to change config and make another algorithm. Or maybe just save the results and rerun and then have another notebook that compares. Then we can bring the differences back here since its the same seed?\n",
    "best_epoch, best_val_metric = load(algorithm, eval_model_path, device=config.device)\n",
    "if config.eval_epoch is None:\n",
    "    epoch = best_epoch\n",
    "else:\n",
    "    epoch = config.eval_epoch\n",
    "if epoch == best_epoch:\n",
    "    is_best = True\n",
    "\n",
    "print(\"evaluating\")\n",
    "evaluate(\n",
    "    algorithm=algorithm,\n",
    "    datasets=datasets,\n",
    "    epoch=epoch,\n",
    "    general_logger=logger,\n",
    "    config=config,\n",
    "    is_best=is_best)\n",
    "\n",
    "# for labeled_batch in train_loader:\n",
    "#     x, y, metadata = labeled_batch\n",
    "\n",
    "#     plt.imshow(x[0].permute(1, 2, 0))\n",
    "#     print('y', y[0])\n",
    "#     print(metadata[0])\n",
    "#     region, year, label, from_source_domain = metadata[0]\n",
    "#     # # year group 0 is 2002  \n",
    "#     # 0:2002, 1:2003, 2: 2004, 3:2005, 4:2006, 5:2007, 6:2008, 7:2009, 8:2010, 9:2011, 10:2012\n",
    "#     print('region', region)\n",
    "#     print('year', year)\n",
    "#     print('label', label)\n",
    "#     print('from_source_domain', from_source_domain)\n",
    "\n",
    "#     z = train_grouper.metadata_to_group(metadata)\n",
    "#     print('Group', z[0])\n",
    "    \n",
    "\n",
    "#     break\n",
    "\n",
    "logger.close()\n",
    "for split in datasets:\n",
    "    datasets[split]['eval_logger'].close()\n",
    "    datasets[split]['algo_logger'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
