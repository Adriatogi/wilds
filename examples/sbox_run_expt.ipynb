{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_expt.py contents\n",
    "\n",
    "## 1) Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.42578125\n"
     ]
    }
   ],
   "source": [
    "import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8755d436b714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import pyBigWig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %timeit bw = pyBigWig.open(\"/users/abalsubr/wilds/examples/data/encode-tfbs_v1.0/DNASE.K562.fc.signal.bigwig\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bw.values('chr1', 10000, 22800, numpy=True)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2334\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bw' is not defined"
     ]
    }
   ],
   "source": [
    "# import pyBigWig\n",
    "# %timeit bw = pyBigWig.open(\"/users/abalsubr/wilds/examples/data/encode-tfbs_v1.0/DNASE.K562.fc.signal.bigwig\")\n",
    "%timeit bw.values('chr1', 10000, 22800, numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The WILDS package is out of date. Your version is 1.0.0, while the latest version is 1.1.0.\n",
      "WARNING:root:The OGB package is out of date. Your version is 1.2.4, while the latest version is 1.3.0.\n"
     ]
    }
   ],
   "source": [
    "import os, csv, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pyBigWig\n",
    "from collections import defaultdict\n",
    "\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool\n",
    "from train import train, evaluate\n",
    "from algorithms.initializer import initialize_algorithm\n",
    "from transforms import initialize_transform\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs='?', const=True, default=False, type=<function parse_bool at 0x7f51ac14f790>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' set default hyperparams in default_hyperparams.py '''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=supported.datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--train_transform', choices=supported.transforms)\n",
    "parser.add_argument('--eval_transform', choices=supported.transforms)\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='target resolution. for example --target_resolution 224 224 for standard resnet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices = supported.losses)\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--algo_log_metric')\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int)\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "config_camelyon = populate_defaults(config_camelyon)\n",
    "\n",
    "argstr_bdd100k = \"--dataset bdd100k --algorithm ERM --root_dir data\"\n",
    "config_bdd100k = parser.parse_args(argstr_bdd100k.split())\n",
    "config_bdd100k = populate_defaults(config_bdd100k)\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode = populate_defaults(config_encode)\n",
    "\n",
    "config = config_camelyon\n",
    "config = config_encode\n",
    "# config = config_bdd100k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(algo_log_metric=None, algorithm='ERM', batch_size=None, coral_penalty_weight=None, dataset='encode-tfbs', dataset_kwargs={}, device=0, distinct_groups=None, download=False, eval_epoch=None, eval_loader='standard', eval_only=False, eval_splits=[], eval_transform=None, evaluate_all_splits=True, frac=1.0, group_dro_step_size=None, groupby_fields=None, irm_lambda=None, irm_penalty_anneal_iters=None, loader_kwargs={'num_workers': 1, 'pin_memory': True}, log_dir='./logs', log_every=50, loss_function=None, lr=None, max_grad_norm=None, max_token_length=None, model=None, model_kwargs={'pretrained': False}, n_epochs=None, n_groups_per_batch=None, no_group_logging=None, optimizer=None, optimizer_kwargs={'momentum': 0.9}, progress_bar=False, resize_scale=None, resume=False, root_dir='data', save_best=True, save_last=True, save_step=None, scheduler=None, scheduler_kwargs={}, scheduler_metric_name=None, scheduler_metric_split='val', seed=0, split_scheme=None, target_resolution=None, train_loader=None, train_transform=None, uniform_over_groups=None, use_wandb=False, val_metric=None, val_metric_decreasing=None, weight_decay=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "# argstr_camelyon = \"--dataset civilcomments --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.optimizer_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encode-tfbs\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Loader kwargs: {'num_workers': 1, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 2\n",
      "Batch size: 64\n",
      "Eval loader: standard\n",
      "Model: leopard\n",
      "Model kwargs: {'pretrained': False}\n",
      "Train transform: None\n",
      "Eval transform: None\n",
      "Target resolution: None\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Loss function: multitask_bce\n",
      "Groupby fields: ['celltype']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: None\n",
      "Irm lambda: None\n",
      "Irm penalty anneal iters: None\n",
      "Algo log metric: multitask_avgprec\n",
      "Val metric: acc_avg\n",
      "Val metric decreasing: False\n",
      "N epochs: 5\n",
      "Optimizer: Adam\n",
      "Lr: 0.001\n",
      "Weight decay: 0.01\n",
      "Max grad norm: None\n",
      "Optimizer kwargs: {}\n",
      "Scheduler: None\n",
      "Scheduler kwargs: {}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: cuda:0\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "No group logging: False\n",
      "Use wandb: False\n",
      "Progress bar: False\n",
      "Resume: False\n",
      "\n",
      "chr3 3.016324281692505\n",
      "chr2 6.676640510559082\n",
      "chr1 10.41373872756958\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "config.device = torch.device(\"cuda:\" + str(config.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "## Initialize logs\n",
    "if os.path.exists(config.log_dir) and config.resume:\n",
    "    resume=True\n",
    "    mode='a'\n",
    "elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "    resume=False\n",
    "    mode='a'\n",
    "else:\n",
    "    resume=False\n",
    "    mode='w'\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "\n",
    "# Record config\n",
    "log_config(config, logger)\n",
    "\n",
    "# Set random seed\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Data\n",
    "full_dataset = supported.datasets[config.dataset](\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# To implement data augmentation (i.e., have different transforms\n",
    "# at training time vs. test time), modify these two lines:\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.train_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.eval_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize dataset object (trial version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3 3.0039219856262207\n",
      "chr4 5.89985990524292\n",
      "chr5 8.640583038330078\n",
      "chr6 11.237342596054077\n",
      "chr7 13.666043519973755\n",
      "chr10 15.858035326004028\n",
      "chr12 17.94972252845764\n",
      "chr13 19.689449071884155\n",
      "chr14 21.30842876434326\n",
      "chr15 22.856398582458496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9228d918e1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0m_seq_bp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchrom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_all_chroms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_seq_bp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mitime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    255\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                                          pickle_kwargs=self.pickle_kwargs)\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    777\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "root_dir='data'\n",
    "download=False\n",
    "split_scheme='official'\n",
    "\n",
    "itime = time.time()\n",
    "_dataset_name = 'encode-tfbs'\n",
    "_version = '1.0'\n",
    "_download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "_data_dir = 'data/encode-tfbs_v1.0/'\n",
    "_y_size = 1\n",
    "_n_classes = 2\n",
    "\n",
    "_train_chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr10', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "_val_chroms = ['chr2', 'chr9', 'chr11']\n",
    "_test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "_transcription_factor = 'MAX'\n",
    "_train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "_val_celltype = ['A549']\n",
    "_test_celltype = ['GM12878']\n",
    "_all_chroms = _train_chroms + _val_chroms + _test_chroms\n",
    "_all_celltypes = _train_celltypes + _val_celltype + _test_celltype\n",
    "\n",
    "_metadata_map = {}\n",
    "_metadata_map['chr'] = _all_chroms\n",
    "_metadata_map['celltype'] = _all_celltypes\n",
    "\n",
    "# Get the splits\n",
    "if split_scheme=='official':\n",
    "    split_scheme = 'standard'\n",
    "\n",
    "_split_scheme = split_scheme\n",
    "_split_dict = {\n",
    "    'train': 0,\n",
    "    'id_val': 1,\n",
    "    'test': 2,\n",
    "    'val': 3\n",
    "}\n",
    "_split_names = {\n",
    "    'train': 'Train',\n",
    "    'id_val': 'Validation (ID)',\n",
    "    'test': 'Test',\n",
    "    'val': 'Validation (OOD)',\n",
    "}\n",
    "\n",
    "# Load sequence and DNase features\n",
    "sequence_filename = os.path.join(_data_dir, 'sequence.npz')\n",
    "seq_arr = np.load(sequence_filename)\n",
    "_seq_bp = {}\n",
    "for chrom in _all_chroms:\n",
    "    _seq_bp[chrom] = seq_arr[chrom]\n",
    "    print(chrom, time.time() - itime)\n",
    "\n",
    "_dnase_allcelltypes = {}\n",
    "ct = 'avg'\n",
    "dnase_avg_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "_dnase_allcelltypes[ct] = pyBigWig.open(dnase_avg_bw_path)\n",
    "for ct in _all_celltypes:\n",
    "    \"\"\"\n",
    "    dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "    dnase_npz_contents = np.load(dnase_filename)\n",
    "    self._dnase_allcelltypes[ct] = {}\n",
    "    for chrom in self._all_chroms: #self._seq_bp:\n",
    "        self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "    \"\"\"\n",
    "    dnase_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "    _dnase_allcelltypes[ct] = pyBigWig.open(dnase_bw_path)\n",
    "    print(ct, time.time() - itime)\n",
    "\n",
    "_metadata_df = pd.read_csv(\n",
    "    _data_dir + 'labels/MAX/metadata_df.bed', sep='\\t', header=None, \n",
    "    index_col=None, names=['chr', 'start', 'stop', 'celltype']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_regions_mask = np.isin(_metadata_df['chr'], _train_chroms)\n",
    "val_regions_mask = np.isin(_metadata_df['chr'], _val_chroms)\n",
    "test_regions_mask = np.isin(_metadata_df['chr'], _test_chroms)\n",
    "train_celltype_mask = np.isin(_metadata_df['celltype'], _train_celltypes)\n",
    "val_celltype_mask = np.isin(_metadata_df['celltype'], _val_celltype)\n",
    "test_celltype_mask = np.isin(_metadata_df['celltype'], _test_celltype)\n",
    "\n",
    "split_array = -1*np.ones(_metadata_df.shape[0]).astype(int)\n",
    "split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = _split_dict['train']\n",
    "split_array[np.logical_and(test_regions_mask, test_celltype_mask)] = _split_dict['test']\n",
    "# Validate using validation chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = _split_dict['val']\n",
    "split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = _split_dict['id_val']\n",
    "\n",
    "if _split_scheme=='standard':\n",
    "    _metadata_df.insert(len(_metadata_df.columns), 'split', split_array)\n",
    "else:\n",
    "    raise ValueError(f'Split scheme {_split_scheme} not recognized')\n",
    "\n",
    "metadata_mask = (_metadata_df['split'] != -1)\n",
    "_metadata_df = _metadata_df[_metadata_df['split'] != -1]\n",
    "\n",
    "chr_ints = _metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['chr'])] )).values\n",
    "celltype_ints = _metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['celltype'])] )).values\n",
    "_split_array = _metadata_df['split'].values\n",
    "\n",
    "_y_array = torch.Tensor(np.load(_data_dir + 'labels/MAX/metadata_y.npy'))\n",
    "_y_array = _y_array[metadata_mask]\n",
    "\n",
    "_metadata_array = torch.stack(\n",
    "    (torch.LongTensor(chr_ints), \n",
    "     torch.LongTensor(celltype_ints)\n",
    "    ),\n",
    "    dim=1)\n",
    "_metadata_fields = ['chr', 'celltype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_random_label_vec(\n",
    "    metadata_df, seed_chr, seed_celltype, seed_start, output_size=128\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a coordinate in a celltype, gets the labels of \n",
    "    the `output_size` 200bp bins from that coordinate onward. \n",
    "    \"\"\"\n",
    "    itime = time.time()\n",
    "    \n",
    "    # Extract regions from this chromosome in this celltype, to get a window of labels from\n",
    "    # print(time.time() - itime)\n",
    "    # chr_msk = np.array(metadata_df['chr']) == seed_region['chr']\n",
    "    # print(time.time() - itime)\n",
    "    # ct_msk = np.array(metadata_df['celltype']) == seed_region['celltype']\n",
    "    # mdf = metadata_df[chr_msk & ct_msk]\n",
    "    seq_size = output_size*50\n",
    "    mdf = metadata_df.loc[\n",
    "        (metadata_df['chr'] == seed_chr) & \n",
    "        (metadata_df['celltype'] == seed_celltype) & \n",
    "        (metadata_df['start'] >= seed_start) & \n",
    "        (metadata_df['stop'] < seed_start+seq_size)\n",
    "    ]\n",
    "    print(time.time() - itime)\n",
    "\n",
    "    # Get labels\n",
    "    y_label_vec = np.zeros(output_size)\n",
    "    y_label_vec[(mdf['start'] - seed_start) // 50] = mdf['y']\n",
    "    return mdf, y_label_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data...\n",
      "    celltype = H1-hESC: n = 5314\n",
      "    celltype = HCT116: n = 4759\n",
      "    celltype = HeLa-S3: n = 4635\n",
      "    celltype = HepG2: n = 4459\n",
      "    celltype = K562: n = 5169\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Validation (ID) data...\n",
      "    celltype = H1-hESC: n = 6872\n",
      "    celltype = HCT116: n = 6315\n",
      "    celltype = HeLa-S3: n = 4219\n",
      "    celltype = HepG2: n = 8356\n",
      "    celltype = K562: n = 6538\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Test data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 4487\n",
      "Validation (OOD) data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 6728\n",
      "    celltype = GM12878: n = 0\n",
      "Dout: 128\n"
     ]
    }
   ],
   "source": [
    "# config = config_encode\n",
    "\n",
    "train_grouper = CombinatorialGrouper(\n",
    "    dataset=full_dataset,\n",
    "    groupby_fields=config.groupby_fields)\n",
    "\n",
    "datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "    # Loggers\n",
    "    # Loggers\n",
    "    datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    if config.use_wandb:\n",
    "        initialize_wandb(config)\n",
    "\n",
    "# Logging dataset info\n",
    "if config.no_group_logging and full_dataset.is_classification and full_dataset.y_size==1:\n",
    "    log_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=['y'])\n",
    "elif config.no_group_logging:\n",
    "    log_grouper = None\n",
    "else:\n",
    "    log_grouper = train_grouper\n",
    "log_group_data(datasets, log_grouper, logger)\n",
    "\n",
    "## Initialize algorithm\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=datasets,\n",
    "    train_grouper=train_grouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in datasets['train']['loader']:\n",
    "    x, y_true, metadata = batch\n",
    "    break\n",
    "# x = torch.transpose(x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7212, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = algorithm.process_batch(batch)\n",
    "\n",
    "a = algorithm.loss.compute(d['y_pred'], d['y_true'], return_dict=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0.5, 0.5, 0.5],\n",
       "       [0. , 0. , 0. , ..., 0.5, 0.5, 1. ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.unique(full_dataset._metadata_df['split'], return_counts=True)\n",
    "y_true.squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cc6c7af60d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "source": [
    "#import importlib\n",
    "importlib.reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0]:\n",
      "\n",
      "Train:\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2432,) [1 0 1 ... 1 1 0] (2432,) 0.09923777357272781 tensor(0.0992, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [1 1 0 ... 1 0 1] (1792,) 0.18020602071676678 tensor(0.1802, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (896,) [1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0\n",
      " 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0\n",
      " 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0\n",
      " 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
      " 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
      " 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
      " 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
      " 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
      " 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0\n",
      " 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 1 1 1 0 1 1 0 1] (896,) 0.12653340353855683 tensor(0.1265, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 1 1 0] (1152,) 0.15009138463477656 tensor(0.1501, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ...  True  True  True] (1920,) [0 0 1 ... 1 0 0] (1920,) 0.13893378955027236 tensor(0.1389, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [1 0 1 ... 1 1 0] (8192,) 0.13583524260280033 tensor(0.1358, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ...  True  True  True] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.07954545454545454 tensor(0.0795, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.11778846153846154 tensor(0.1178, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.08138020833333333 tensor(0.0814, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.196875 tensor(0.1969, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.1623263888888889 tensor(0.1623, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1292724609375 tensor(0.1293, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.09678819444444445 tensor(0.0968, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.19587053571428573 tensor(0.1959, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.1015625 tensor(0.1016, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True  True  True  True False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (512,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (512,) 0.154296875 tensor(0.1543, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.142578125 tensor(0.1426, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1318359375 tensor(0.1318, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2432,) [0 0 0 ... 0 0 0] (2432,) 0.09580592105263158 tensor(0.0958, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.2506510416666667 tensor(0.2507, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.09809027777777778 tensor(0.0981, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True False False ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.2353515625 tensor(0.2354, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.109375 tensor(0.1094, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.14599609375 tensor(0.1460, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ...  True  True  True] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.10107421875 tensor(0.1011, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.20454545454545456 tensor(0.2045, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.099609375 tensor(0.0996, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.19810267857142858 tensor(0.1981, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.10885416666666667 tensor(0.1089, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1417236328125 tensor(0.1417, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.11221590909090909 tensor(0.1122, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.13040865384615385 tensor(0.1304, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ...  True  True  True] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.09588068181818182 tensor(0.0959, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.16471354166666666 tensor(0.1647, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.14935661764705882 tensor(0.1494, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1328125 tensor(0.1328, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.10997596153846154 tensor(0.1100, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.1232638888888889 tensor(0.1233, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.13616071428571427 tensor(0.1362, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.184375 tensor(0.1844, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.14149305555555555 tensor(0.1415, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1380615234375 tensor(0.1381, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.1 tensor(0.1000, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.15980113636363635 tensor(0.1598, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.11160714285714286 tensor(0.1116, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False False False\n",
      " False False False False False False False False False False False False] (768,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (768,) 0.21614583333333334 tensor(0.2161, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2944,) [0 0 0 ... 0 0 0] (2944,) 0.1328125 tensor(0.1328, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.135498046875 tensor(0.1355, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.13385416666666666 tensor(0.1339, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.15178571428571427 tensor(0.1518, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.1203125 tensor(0.1203, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      " False False  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (896,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0] (896,) 0.09040178571428571 tensor(0.0904, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.1045673076923077 tensor(0.1046, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1239013671875 tensor(0.1239, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.13051470588235295 tensor(0.1305, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.14609375 tensor(0.1461, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.12239583333333333 tensor(0.1224, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.14118303571428573 tensor(0.1412, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.16193181818181818 tensor(0.1619, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.13916015625 tensor(0.1392, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False  True  True] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.09801136363636363 tensor(0.0980, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2816,) [0 0 0 ... 0 0 0] (2816,) 0.10404829545454546 tensor(0.1040, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.0875 tensor(0.0875, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.20099431818181818 tensor(0.2010, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.13984375 tensor(0.1398, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1226806640625 tensor(0.1227, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.11145833333333334 tensor(0.1115, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.181640625 tensor(0.1816, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.11484375 tensor(0.1148, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.1658653846153846 tensor(0.1659, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.16685267857142858 tensor(0.1669, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1483154296875 tensor(0.1483, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.1302568958818959 tensor(0.1303, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.189453125 tensor(0.1895, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.11067708333333333 tensor(0.1107, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.23468815928270043 tensor(0.2347, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.13385416666666666 tensor(0.1339, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.16484484726123597 tensor(0.1648, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.11263020833333333 tensor(0.1126, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.15576171875 tensor(0.1558, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.11610949612403101 tensor(0.1161, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.17734375 tensor(0.1773, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.171875 tensor(0.1719, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1438577872555272 tensor(0.1439, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ...  True  True  True] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.12439903846153846 tensor(0.1244, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.22088068181818182 tensor(0.2209, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.15223817567567566 tensor(0.1522, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.1356534090909091 tensor(0.1357, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.15503202814868278 tensor(0.1550, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.15613628135565832 tensor(0.1561, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.08984375 tensor(0.0898, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.14543269230769232 tensor(0.1454, device='cuda:0', dtype=torch.float64)\n",
      "why   [False  True  True ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.1545138888888889 tensor(0.1545, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.16601859327507598 tensor(0.1660, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.09486607142857142 tensor(0.0949, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1307907754109508 tensor(0.1308, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      " False False False  True  True  True  True  True  True False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False] (896,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0] (896,) 0.18861607142857142 tensor(0.1886, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2560,) [0 0 0 ... 0 0 0] (2560,) 0.2031711368110236 tensor(0.2032, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.12560096153846154 tensor(0.1256, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False] (768,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (768,) 0.1171875 tensor(0.1172, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False  True ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.14322916666666666 tensor(0.1432, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.16135470753972053 tensor(0.1614, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.13365334378265414 tensor(0.1337, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.14312537741545892 tensor(0.1431, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.1383054595896147 tensor(0.1383, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.1884765625 tensor(0.1885, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.0889423076923077 tensor(0.0889, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1392934035570018 tensor(0.1393, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False  True] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.1148314123790117 tensor(0.1148, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.234375 tensor(0.2344, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.13518363161819538 tensor(0.1352, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.1484375 tensor(0.1484, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.20069918995700245 tensor(0.2007, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.16518310916225415 tensor(0.1652, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.14157774390243902 tensor(0.1416, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.16829982517482517 tensor(0.1683, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1536,) [0 0 0 ... 0 1 1] (1536,) 0.12203414351851852 tensor(0.1220, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.14015534682080924 tensor(0.1402, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.18266864778921865 tensor(0.1827, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.15107465864301803 tensor(0.1511, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.12805550230061352 tensor(0.1281, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.1798145077383275 tensor(0.1798, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1536,) [0 0 0 ... 1 0 0] (1536,) 0.14846865031897927 tensor(0.1485, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.2182291666666667 tensor(0.2182, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.17879293893129775 tensor(0.1788, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.17408185325186412 tensor(0.1741, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.13385180995475113 tensor(0.1339, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.1634497549019608 tensor(0.1634, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.2111472315436242 tensor(0.2111, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.17961774553571427 tensor(0.1796, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.19505408546397282 tensor(0.1951, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.17720760641838973 tensor(0.1772, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.17260322523480418 tensor(0.1726, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.19631456413210446 tensor(0.1963, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.16002286585365852 tensor(0.1600, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.15676843030872636 tensor(0.1568, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.19980746809032893 tensor(0.1998, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.17694871945488722 tensor(0.1769, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ...  True  True False] (3200,) [0 0 0 ... 0 0 0] (3200,) 0.17646062940470833 tensor(0.1765, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.21987862976406533 tensor(0.2199, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1536,) [1 0 0 ... 0 0 0] (1536,) 0.22485079470618036 tensor(0.2249, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.18892249103942654 tensor(0.1889, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.20539447623239437 tensor(0.2054, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ...  True  True False] (8192,) [1 0 0 ... 0 0 0] (8192,) 0.1956759851363835 tensor(0.1957, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [ True  True  True ... False False False] (2560,) [1 1 1 ... 0 0 0] (2560,) 0.16270833333333334 tensor(0.1627, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1536,) [0 0 0 ... 1 0 0] (1536,) 0.28461934747103557 tensor(0.2846, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.2885416666666667 tensor(0.2885, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.24493883087633087 tensor(0.2449, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.1776813162682728 tensor(0.1777, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [1 1 1 ... 0 0 0] (8192,) 0.22326946266948078 tensor(0.2233, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.19251085890430153 tensor(0.1925, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.22502709178398156 tensor(0.2250, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.22283878504672897 tensor(0.2228, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.2044723429144385 tensor(0.2045, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.1890666335978836 tensor(0.1891, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.1970471833881579 tensor(0.1970, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.17814201811043567 tensor(0.1781, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.22176106178589622 tensor(0.2218, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.15586979984301413 tensor(0.1559, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.19933712121212122 tensor(0.1993, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.27441314553990614 tensor(0.2744, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.20748284786370724 tensor(0.2075, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.1627858889528193 tensor(0.1628, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.3289409447955064 tensor(0.3289, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1408,) [0 1 0 ... 0 0 0] (1408,) 0.25750782574670666 tensor(0.2575, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.2380265050832091 tensor(0.2380, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.20203645462301223 tensor(0.2020, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.2304055108248235 tensor(0.2304, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [ True  True  True ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.18352952167414052 tensor(0.1835, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.391332129896404 tensor(0.3913, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (896,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0] (896,) 0.43876971003366205 tensor(0.4388, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.26807482215447154 tensor(0.2681, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.2458394306739895 tensor(0.2458, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.2842815311314583 tensor(0.2843, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.24575731426692965 tensor(0.2458, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.276717519724741 tensor(0.2767, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2048,) [0 0 0 ... 0 0 0] (2048,) 0.38168526600954644 tensor(0.3817, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (896,) [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0] (896,) 0.3275530937683716 tensor(0.3276, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.24250047241118666 tensor(0.2425, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.27537596564595973 tensor(0.2754, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.339521139314602 tensor(0.3395, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.28316756119010217 tensor(0.2832, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1024,) [0 1 0 ... 0 0 0] (1024,) 0.30224860634648365 tensor(0.3022, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.23057474330872174 tensor(0.2306, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.22791799898259513 tensor(0.2279, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.27437629915291323 tensor(0.2744, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.21319969405140976 tensor(0.2132, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.3474399687036469 tensor(0.3474, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.19751082251082253 tensor(0.1975, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1152,) [0 0 0 ... 0 0 0] (1152,) 0.3353790123844628 tensor(0.3354, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.24501893939393937 tensor(0.2450, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.2624466475767001 tensor(0.2624, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [1 1 1 ... 0 0 0] (1408,) 0.22450973341004987 tensor(0.2245, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.27231664754255114 tensor(0.2723, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.3152901785714286 tensor(0.3153, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.35922695360195356 tensor(0.3592, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ...  True  True  True] (2432,) [0 0 0 ... 0 0 0] (2432,) 0.26736473289421736 tensor(0.2674, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [1 1 1 ... 0 0 0] (8192,) 0.28538833123099405 tensor(0.2854, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.245172509039775 tensor(0.2452, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.24340502699055327 tensor(0.2434, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.28707033026885964 tensor(0.2871, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.2957705135233918 tensor(0.2958, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.2895262781476896 tensor(0.2895, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.2656280862586716 tensor(0.2656, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ...  True  True False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.19391790985177615 tensor(0.1939, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1152,) [0 0 0 ... 1 1 1] (1152,) 0.39839248075956224 tensor(0.3984, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.3400271739130435 tensor(0.3400, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.26218694096601075 tensor(0.2622, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (2304,) [0 0 0 ... 0 0 0] (2304,) 0.25949223766281415 tensor(0.2595, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.2731843170244799 tensor(0.2732, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2432,) [0 0 0 ... 0 0 0] (2432,) 0.23153263758670284 tensor(0.2315, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.3294548915822105 tensor(0.3295, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1024,) [0 0 0 ... 0 0 0] (1024,) 0.50768331438611 tensor(0.5077, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.17941607556456285 tensor(0.1794, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.4005733735380117 tensor(0.4006, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.32525391000796444 tensor(0.3253, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.28327316031926486 tensor(0.2833, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (2688,) [0 0 0 ... 0 0 0] (2688,) 0.2455340291329215 tensor(0.2455, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False] (896,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0] (896,) 0.36439732142857145 tensor(0.3644, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False] (768,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] (768,) 0.36334134615384617 tensor(0.3633, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.36328125 tensor(0.3633, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.3073375105806347 tensor(0.3073, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.42102430988608963 tensor(0.4210, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.39475473771436803 tensor(0.3948, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.3678160635096611 tensor(0.3678, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.19351388184584178 tensor(0.1935, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1792,) [0 0 0 ... 0 0 0] (1792,) 0.24591191813804175 tensor(0.2459, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.3096451568959731 tensor(0.3096, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.24075629195519133 tensor(0.2408, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1536,) [0 0 0 ... 0 0 0] (1536,) 0.17249526515151514 tensor(0.1725, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.2863095238095238 tensor(0.2863, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.4012790080941676 tensor(0.4013, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (1920,) [0 0 0 ... 0 0 0] (1920,) 0.32623064828253506 tensor(0.3262, device='cuda:0', dtype=torch.float64)\n",
      "why   [ True  True  True ... False False False] (8192,) [0 0 0 ... 0 0 0] (8192,) 0.29373969403168476 tensor(0.2937, device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([8192]) torch.Size([8192]) tensor(8064, device='cuda:0') torch.Size([64, 128]) torch.Size([64, 128])\n",
      "why   [False False False ... False False False] (2176,) [0 0 0 ... 0 0 0] (2176,) 0.3421500286608995 tensor(0.3422, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (1664,) [0 0 0 ... 1 1 0] (1664,) 0.22848216513818703 tensor(0.2285, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1280,) [0 0 0 ... 0 0 0] (1280,) 0.21294610507246378 tensor(0.2129, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1408,) [0 0 0 ... 0 0 0] (1408,) 0.4324312010246706 tensor(0.4324, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ... False False False] (1664,) [0 0 0 ... 0 0 0] (1664,) 0.42839099459862173 tensor(0.4284, device='cuda:0', dtype=torch.float64)\n",
      "why   [False False False ...  True  True  True] (8192,) [0 0 0 ... 1 1 0] (8192,) 0.3411826173375903 tensor(0.3412, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6a6bdb2f3d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbest_val_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(algorithm, datasets, general_logger, config, epoch_offset, best_val_metric)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# First run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneral_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Then run val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/train.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(algorithm, dataset, general_logger, epoch, config, train)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/single_model_algorithm.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# process batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m# log results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/single_model_algorithm.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         self.step_schedulers(\n\u001b[1;32m    124\u001b[0m             \u001b[0mis_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not config.eval_only:\n",
    "    ## Load saved results if resuming\n",
    "    resume_success = False\n",
    "    if resume:\n",
    "        save_path = os.path.join(config.log_dir, 'last_model.pth')\n",
    "        if not os.path.exists(save_path):\n",
    "            epochs = [\n",
    "                int(file.split('_')[0])\n",
    "                for file in os.listdir(config.log_dir) if file.endswith('.pth')]\n",
    "            if len(epochs) > 0:\n",
    "                latest_epoch = max(epochs)\n",
    "                save_path = os.path.join(config.log_dir, f'{latest_epoch}_model.pth')\n",
    "        try:\n",
    "            prev_epoch, best_val_metric = load(algorithm, save_path)\n",
    "            epoch_offset = prev_epoch + 1\n",
    "            logger.write(f'Resuming from epoch {epoch_offset} with best val metric {best_val_metric}')\n",
    "            resume_success = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    if resume_success == False:\n",
    "        epoch_offset=0\n",
    "        best_val_metric=None\n",
    "    \n",
    "    train(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        general_logger=logger,\n",
    "        config=config,\n",
    "        epoch_offset=epoch_offset,\n",
    "        best_val_metric=best_val_metric)\n",
    "else:\n",
    "    if config.eval_epoch is None:\n",
    "        eval_model_path = os.path.join(config.log_dir, 'best_model.pth')\n",
    "    else:\n",
    "        eval_model_path = os.path.join(config.log_dir, f'{config.eval_epoch}_model.pth')\n",
    "    best_epoch, best_val_metric = load(algorithm, eval_model_path)\n",
    "    if config.eval_epoch is None:\n",
    "        epoch = best_epoch\n",
    "    else:\n",
    "        epoch = config.eval_epoch\n",
    "    evaluate(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        epoch=epoch,\n",
    "        general_logger=logger,\n",
    "        config=config)\n",
    "\n",
    "logger.close()\n",
    "for split in datasets:\n",
    "    datasets[split]['eval_logger'].close()\n",
    "    datasets[split]['algo_logger'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
