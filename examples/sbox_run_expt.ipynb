{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_expt.py contents\n",
    "\n",
    "## 1) Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-7360ca071eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psutil'"
     ]
    }
   ],
   "source": [
    "import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 µs ± 343 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import pyBigWig\n",
    "# %timeit bw = pyBigWig.open(\"/users/abalsubr/wilds/examples/data/encode-tfbs_v1.0/DNASE.K562.fc.signal.bigwig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.2.4, while the latest version is 1.2.5.\n"
     ]
    }
   ],
   "source": [
    "import os, csv\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool\n",
    "from train import train, evaluate\n",
    "from algorithms.initializer import initialize_algorithm\n",
    "from transforms import initialize_transform\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs='?', const=True, default=False, type=<function parse_bool at 0x7f47e44395e0>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' set default hyperparams in default_hyperparams.py '''\n",
    "parser = argparse.ArgumentParser()\n",
    "CombinatorialGrouper\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=supported.datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--train_transform', choices=supported.transforms)\n",
    "parser.add_argument('--eval_transform', choices=supported.transforms)\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='target resolution. for example --target_resolution 224 224 for standard resnet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices = supported.losses)\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--algo_log_metric')\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int)\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "config_camelyon = populate_defaults(config_camelyon)\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode = populate_defaults(config_encode)\n",
    "\n",
    "# config = config_camelyon\n",
    "config = config_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encode-tfbs\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Loader kwargs: {'num_workers': 4, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 2\n",
      "Batch size: 64\n",
      "Eval loader: standard\n",
      "Model: leopard\n",
      "Model kwargs: {'pretrained': False}\n",
      "Train transform: None\n",
      "Eval transform: None\n",
      "Target resolution: None\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Loss function: cross_entropy\n",
      "Groupby fields: ['celltype', 'y']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: None\n",
      "Irm lambda: None\n",
      "Irm penalty anneal iters: None\n",
      "Algo log metric: accuracy\n",
      "Val metric: acc_avg\n",
      "Val metric decreasing: False\n",
      "N epochs: 5\n",
      "Optimizer: Adam\n",
      "Lr: 0.001\n",
      "Weight decay: 0.01\n",
      "Max grad norm: None\n",
      "Optimizer kwargs: {'momentum': 0.9}\n",
      "Scheduler: None\n",
      "Scheduler kwargs: {}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: cuda:0\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "No group logging: False\n",
      "Use wandb: False\n",
      "Progress bar: False\n",
      "Resume: False\n",
      "\n",
      "chr2 3.6633927822113037\n",
      "chr3 6.7115819454193115\n",
      "chr4 9.648637771606445\n",
      "chr5 12.439441919326782\n",
      "chr6 15.091757774353027\n",
      "chr7 17.542895555496216\n",
      "chr9 19.707583904266357\n",
      "chr10 21.79905652999878\n",
      "chr11 23.86957049369812\n",
      "chr12 25.918642044067383\n",
      "chr13 27.675577402114868\n",
      "chr14 29.3148353099823\n",
      "chr15 30.881144046783447\n",
      "chr16 32.271193504333496\n",
      "chr17 33.51785063743591\n",
      "chr18 34.72123050689697\n",
      "chr19 35.627156257629395\n",
      "chr20 36.59872794151306\n",
      "chr22 37.37847852706909\n",
      "chrX 39.77280807495117\n",
      "chr1 43.60475468635559\n",
      "chr8 45.86070203781128\n",
      "chr21 46.59553360939026\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_all_celltypes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c13ba58136d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m full_dataset = supported.datasets[config.dataset](\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/wilds/datasets/encodetfbs_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, download, split_scheme)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Get the y values, and remove ambiguous labels by default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpd_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_all_celltypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mtc_chr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtc_chr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_all_celltypes' is not defined"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "config.device = torch.device(\"cuda:\" + str(config.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "## Initialize logs\n",
    "if os.path.exists(config.log_dir) and config.resume:\n",
    "    resume=True\n",
    "    mode='a'\n",
    "elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "    resume=False\n",
    "    mode='a'\n",
    "else:\n",
    "    resume=False\n",
    "    mode='w'\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "\n",
    "# Record config\n",
    "log_config(config, logger)\n",
    "\n",
    "# Set random seed\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Data\n",
    "full_dataset = supported.datasets[config.dataset](\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# To implement data augmentation (i.e., have different transforms\n",
    "# at training time vs. test time), modify these two lines:\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.train_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.eval_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wilds.datasets.camelyon17_dataset.Camelyon17Dataset at 0x7f99b84f3e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "full_dataset_camelyon17 = copy.deepcopy(full_dataset)\n",
    "\n",
    "# supported.datasets[config_encode.dataset]\n",
    "# print(config_camelyon.train_transform, config_encode.train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a21083194cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 3.90254282951355\n",
      "chr9 6.149690628051758\n",
      "chr11 8.327073097229004\n",
      "chr1 12.291624546051025\n",
      "chr8 14.624409675598145\n",
      "chr21 15.413429021835327\n",
      "H1-hESC 15.415196895599365\n",
      "HCT116 15.415941953659058\n",
      "HeLa-S3 15.416455030441284\n",
      "HepG2 15.417592763900757\n",
      "K562 15.418397426605225\n",
      "A549 15.41891360282898\n",
      "GM12878 15.419732332229614\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "root_dir='data'\n",
    "download=False\n",
    "split_scheme='official'\n",
    "\n",
    "itime = time.time()\n",
    "_dataset_name = 'encode-tfbs'\n",
    "_version = '1.0'\n",
    "_download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "_data_dir = 'data/encode-tfbs_v1.0'\n",
    "_y_size = 1\n",
    "_n_classes = 2\n",
    "\n",
    "# _train_chroms = ['chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "_train_chroms = ['chr2', 'chr9', 'chr11']\n",
    "_test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "_transcription_factor = 'MAX'\n",
    "_train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "_val_celltype = ['A549']\n",
    "_test_celltype = ['GM12878']\n",
    "_all_chroms = _train_chroms + _test_chroms\n",
    "_all_celltypes = _train_celltypes + _val_celltype + _test_celltype\n",
    "\n",
    "_metadata_map = {}\n",
    "_metadata_map['chr'] = _all_chroms\n",
    "_metadata_map['celltype'] = _all_celltypes\n",
    "\n",
    "# Get the splits\n",
    "if split_scheme=='official':\n",
    "    split_scheme = 'standard'\n",
    "\n",
    "_split_scheme = split_scheme\n",
    "_split_dict = {\n",
    "    'train': 0,\n",
    "    'id_val': 1,\n",
    "    'test': 2,\n",
    "    'val': 3\n",
    "}\n",
    "_split_names = {\n",
    "    'train': 'Train',\n",
    "    'id_val': 'Validation (ID)',\n",
    "    'test': 'Test',\n",
    "    'val': 'Validation (OOD)',\n",
    "}\n",
    "\n",
    "# Load sequence and DNase features\n",
    "sequence_filename = os.path.join(_data_dir, 'sequence.npz')\n",
    "seq_arr = np.load(sequence_filename)\n",
    "_seq_bp = {}\n",
    "for chrom in _all_chroms: #seq_arr:\n",
    "    _seq_bp[chrom] = seq_arr[chrom]\n",
    "    print(chrom, time.time() - itime)\n",
    "\n",
    "_dnase_allcelltypes = {}\n",
    "ct = 'avg'\n",
    "dnase_avg_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "_dnase_allcelltypes[ct] = pyBigWig.open(dnase_avg_bw_path)\n",
    "for ct in _all_celltypes:\n",
    "    \"\"\"\n",
    "    dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "    dnase_npz_contents = np.load(dnase_filename)\n",
    "    self._dnase_allcelltypes[ct] = {}\n",
    "    for chrom in self._all_chroms: #self._seq_bp:\n",
    "        self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "    \"\"\"\n",
    "    dnase_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "    _dnase_allcelltypes[ct] = pyBigWig.open(dnase_bw_path)\n",
    "    print(ct, time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.06488299369812\n"
     ]
    }
   ],
   "source": [
    "itime = time.time()\n",
    "\n",
    "# Read in metadata dataframe from training+validation data\n",
    "train_regions_labeled = pd.read_csv(os.path.join(_data_dir, 'labels/{}.train.labels.tsv.gz'.format(_transcription_factor)), sep='\\t')\n",
    "val_regions_labeled = pd.read_csv(os.path.join(_data_dir, 'labels/{}.val.labels.tsv.gz'.format(_transcription_factor)), sep='\\t')\n",
    "training_df = train_regions_labeled[np.isin(train_regions_labeled['chr'], _train_chroms)]\n",
    "val_df = val_regions_labeled[np.isin(val_regions_labeled['chr'], _test_chroms)]\n",
    "all_df = pd.concat([training_df, val_df])\n",
    "\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-523b6cd170e9>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tc_chr['y'] = tc_chr['y'].replace({'U': 0, 'B': 1, 'A': 0.5}).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.203907012939453\n",
      "22.746795415878296\n",
      "32.076903104782104\n",
      "41.43525505065918\n",
      "51.37267017364502\n",
      "61.07364773750305\n",
      "71.10506510734558\n",
      "100.72125267982483\n"
     ]
    }
   ],
   "source": [
    "itime = time.time()\n",
    "\n",
    "# Get the y values, and remove ambiguous labels by default.\n",
    "pd_list = []\n",
    "for ct in _all_celltypes:\n",
    "    tc_chr = all_df[['chr', 'start', 'stop', ct]]\n",
    "    tc_chr.columns = ['chr', 'start', 'stop', 'y']\n",
    "    tc_chr['y'] = tc_chr['y'].replace({'U': 0, 'B': 1, 'A': 0.5}).values\n",
    "    \n",
    "    # # Now filter out ambiguous labels\n",
    "    # non_ambig_mask = (y_array != -1)\n",
    "    # tc_chr['y'] = y_array\n",
    "    # tc_chr = tc_chr[non_ambig_mask]\n",
    "    \n",
    "    tc_chr.insert(len(tc_chr.columns), 'celltype', ct)\n",
    "    pd_list.append(tc_chr)\n",
    "    print(time.time() - itime)\n",
    "metadata_df = pd.concat(pd_list)\n",
    "\n",
    "print(time.time() - itime)\n",
    "\n",
    "_metadata_df = metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1-hESC 8.80908489227295\n",
      "H1-hESC 12.474784135818481\n",
      "H1-hESC 15.258162498474121\n",
      "HCT116 23.023517370224\n",
      "HCT116 25.439095735549927\n",
      "HCT116 27.21790099143982\n",
      "HeLa-S3 34.81065845489502\n",
      "HeLa-S3 35.2776780128479\n",
      "HeLa-S3 36.60090255737305\n",
      "HepG2 44.36072087287903\n",
      "HepG2 44.74501991271973\n",
      "HepG2 46.02569603919983\n",
      "K562 53.825233697891235\n",
      "K562 54.182188749313354\n",
      "K562 55.44522547721863\n",
      "A549 62.980581283569336\n",
      "A549 63.34522008895874\n",
      "A549 64.59721446037292\n",
      "GM12878 72.41460752487183\n",
      "GM12878 72.7955391407013\n",
      "GM12878 74.05369997024536\n"
     ]
    }
   ],
   "source": [
    "# np.unique(_metadata_df['y'])\n",
    "\n",
    "# Downsample negatives to balance each celltype\n",
    "samp_ndces = []\n",
    "itime = time.time()\n",
    "neg_msk = (_metadata_df['y'] == 0)\n",
    "pos_msk = (_metadata_df['y'] != 0)\n",
    "for ct in _all_celltypes:\n",
    "    celltype_msk = (_metadata_df['celltype'] == ct)\n",
    "    print(ct, time.time() - itime)\n",
    "    neg_ct_msk = np.logical_and(celltype_msk, neg_msk)\n",
    "    pos_ct_msk = np.logical_and(celltype_msk, pos_msk)\n",
    "    print(ct, time.time() - itime)\n",
    "    neg_ndces = np.where(neg_ct_msk)[0]\n",
    "    pos_ndces = np.where(pos_ct_msk)[0]\n",
    "    np.random.seed(42)\n",
    "    samp_neg_ndces = np.random.choice(neg_ndces, size=len(pos_ndces), replace=False)\n",
    "    samp_ndces.extend(samp_neg_ndces)\n",
    "    samp_ndces.extend(pos_ndces)\n",
    "    print(ct, time.time() - itime)\n",
    "_metadata_df = _metadata_df.iloc[samp_ndces, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_label_vec(metadata_df, output_size=128):\n",
    "    # Sample a positively labeled region at random\n",
    "    pos_mdf = metadata_df[metadata_df['y'] == 1] #.iloc[ metadata_df['chr'] == s['chr'], : ]\n",
    "    pos_seed_region = pos_mdf.iloc[np.random.randint(pos_mdf.shape[0])]\n",
    "\n",
    "    # Extract regions from this chromosome in this celltype, to get a window of labels from\n",
    "    chr_msk = np.array(metadata_df['chr']) == pos_seed_region['chr']\n",
    "    ct_msk = np.array(metadata_df['celltype']) == pos_seed_region['celltype']\n",
    "    mdf = metadata_df[chr_msk & ct_msk]\n",
    "\n",
    "    # Get labels\n",
    "    start_ndx = np.where(mdf['start'] == pos_seed_region['start'])[0][0]\n",
    "    y_label_vec = mdf.iloc[start_ndx:start_ndx+output_size, :]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>y</th>\n",
       "      <th>celltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5937924</th>\n",
       "      <td>chr8</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937925</th>\n",
       "      <td>chr8</td>\n",
       "      <td>650</td>\n",
       "      <td>850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937926</th>\n",
       "      <td>chr8</td>\n",
       "      <td>700</td>\n",
       "      <td>900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937927</th>\n",
       "      <td>chr8</td>\n",
       "      <td>750</td>\n",
       "      <td>950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937928</th>\n",
       "      <td>chr8</td>\n",
       "      <td>800</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843006</th>\n",
       "      <td>chr8</td>\n",
       "      <td>146363200</td>\n",
       "      <td>146363400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843007</th>\n",
       "      <td>chr8</td>\n",
       "      <td>146363250</td>\n",
       "      <td>146363450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843008</th>\n",
       "      <td>chr8</td>\n",
       "      <td>146363300</td>\n",
       "      <td>146363500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843009</th>\n",
       "      <td>chr8</td>\n",
       "      <td>146363350</td>\n",
       "      <td>146363550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8843010</th>\n",
       "      <td>chr8</td>\n",
       "      <td>146363400</td>\n",
       "      <td>146363600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HepG2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2905087 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chr      start       stop    y celltype\n",
       "5937924  chr8        600        800  0.0    HepG2\n",
       "5937925  chr8        650        850  0.0    HepG2\n",
       "5937926  chr8        700        900  0.0    HepG2\n",
       "5937927  chr8        750        950  0.0    HepG2\n",
       "5937928  chr8        800       1000  0.0    HepG2\n",
       "...       ...        ...        ...  ...      ...\n",
       "8843006  chr8  146363200  146363400  0.0    HepG2\n",
       "8843007  chr8  146363250  146363450  0.0    HepG2\n",
       "8843008  chr8  146363300  146363500  0.0    HepG2\n",
       "8843009  chr8  146363350  146363550  0.0    HepG2\n",
       "8843010  chr8  146363400  146363600  0.0    HepG2\n",
       "\n",
       "[2905087 rows x 5 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8809571    1.0\n",
       "8809572    1.0\n",
       "8809573    1.0\n",
       "8809574    1.0\n",
       "8809575    1.0\n",
       "8809576    0.5\n",
       "8809577    0.5\n",
       "8809578    0.5\n",
       "8809579    1.0\n",
       "8809580    1.0\n",
       "8809581    1.0\n",
       "8809582    1.0\n",
       "8809583    1.0\n",
       "8809584    1.0\n",
       "8809585    0.5\n",
       "8809586    0.5\n",
       "8809587    0.0\n",
       "8809588    0.0\n",
       "8809589    0.0\n",
       "8809590    0.0\n",
       "8809591    0.0\n",
       "8809592    0.0\n",
       "8809593    0.0\n",
       "8809594    0.0\n",
       "8809595    0.0\n",
       "8809596    0.0\n",
       "8809597    0.0\n",
       "8809598    0.0\n",
       "8809599    0.0\n",
       "8809600    0.0\n",
       "8809601    0.0\n",
       "8809602    0.0\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mdf[:256+start_bin]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chr              chr8\n",
       "start       144691450\n",
       "stop        144691650\n",
       "y                 1.0\n",
       "celltype        HepG2\n",
       "Name: 8809571, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seed_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      600,       600,       600, ..., 135005900, 135005900,\n",
       "       135005900])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arr = metadata_df[mdf_msk]['start']\n",
    "#arr == \n",
    "np.sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.39193439483643\n"
     ]
    }
   ],
   "source": [
    "itime = time.time()\n",
    "lts = ['{}:{}-{}'.format(x[0], x[1], x[2]) for x in zip(metadata_df['chr'], metadata_df['start'], metadata_df['stop'])]\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202800"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_metadata_df['y']\n",
    "# s = metadata_df.iloc[np.array(pos_msk), :]\n",
    "ntry = s.iloc[5]\n",
    "ntry['start'] + 12800\n",
    "# s['chr'], s['start'], s['stop'] # np.unique(s['chr'], return_counts=True)\n",
    "# all_df\n",
    "# metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1-hESC 7.871182918548584\n",
      "H1-hESC 8.298529148101807\n",
      "H1-hESC 9.57175898551941\n",
      "HCT116 17.01794719696045\n",
      "HCT116 17.36267113685608\n",
      "HCT116 18.669682025909424\n",
      "HeLa-S3 26.405478954315186\n",
      "HeLa-S3 26.759119272232056\n",
      "HeLa-S3 28.043395042419434\n",
      "HepG2 35.623862981796265\n",
      "HepG2 35.98245143890381\n",
      "HepG2 37.29869079589844\n",
      "K562 44.92080807685852\n",
      "K562 45.256179332733154\n",
      "K562 46.7364935874939\n",
      "A549 54.39264512062073\n",
      "A549 54.74424934387207\n",
      "A549 56.03351712226868\n",
      "GM12878 63.745240211486816\n",
      "GM12878 64.1029920578003\n",
      "GM12878 65.43286633491516\n"
     ]
    }
   ],
   "source": [
    "train_regions_mask = np.isin(_metadata_df['chr'], _train_chroms)\n",
    "val_regions_mask = np.isin(_metadata_df['chr'], _test_chroms)\n",
    "train_celltype_mask = np.isin(_metadata_df['celltype'], _train_celltypes)\n",
    "val_celltype_mask = np.isin(_metadata_df['celltype'], _val_celltype)\n",
    "test_celltype_mask = np.isin(_metadata_df['celltype'], _test_celltype)\n",
    "\n",
    "split_array = -1*np.ones(_metadata_df.shape[0]).astype(int)\n",
    "split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = _split_dict['train']\n",
    "split_array[np.logical_and(val_regions_mask, test_celltype_mask)] = _split_dict['test']\n",
    "# Validate using test chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = _split_dict['val']\n",
    "split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = _split_dict['id_val']\n",
    "\n",
    "if _split_scheme=='standard':\n",
    "    _metadata_df.insert(len(_metadata_df.columns), 'split', split_array)\n",
    "else:\n",
    "    raise ValueError(f'Split scheme {_split_scheme} not recognized')\n",
    "\n",
    "_metadata_df = _metadata_df[_metadata_df['split'] != -1]\n",
    "_split_array = _metadata_df['split'].values\n",
    "\n",
    "chr_ints = _metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['chr'])] )).values\n",
    "celltype_ints = _metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['celltype'])] )).values\n",
    "_y_array = torch.LongTensor(np.array(_metadata_df['y']))\n",
    "\n",
    "_metadata_array = torch.stack(\n",
    "    (torch.LongTensor(chr_ints), \n",
    "     torch.LongTensor(celltype_ints), \n",
    "     _y_array),\n",
    "    dim=1)\n",
    "_metadata_fields = ['chr', 'celltype', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset object (long version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "class EncodeTFBSDataset(WILDSDataset):\n",
    "    \"\"\"\n",
    "    ENCODE-DREAM-wilds dataset of transcription factor binding sites. \n",
    "    This is a subset of the dataset from the ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge. \n",
    "    \n",
    "    Input (x):\n",
    "        1000-base-pair regions of sequence with a quantified chromatin accessibility readout.\n",
    "\n",
    "    Label (y):\n",
    "        y is binary. It is 1 if the central 200bp region is bound by the transcription factor MAX, and 0 otherwise.\n",
    "\n",
    "    Metadata:\n",
    "        Each sequence is annotated with the celltype of origin (a string) and the chromosome of origin (a string).\n",
    "    \n",
    "    Website:\n",
    "        https://www.synapse.org/#!Synapse:syn6131484\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir='data', download=False, split_scheme='official'):\n",
    "        itime = time.time()\n",
    "        self._dataset_name = 'encode-tfbs'\n",
    "        self._version = '1.0'\n",
    "        self._download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "        self._data_dir = self.initialize_data_dir(root_dir, download)\n",
    "        self._y_size = 1\n",
    "        self._n_classes = 2\n",
    "        \n",
    "        # self._train_chroms = ['chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "        self._train_chroms = ['chr2', 'chr9', 'chr11']\n",
    "        self._test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "        self._transcription_factor = 'MAX'\n",
    "        self._train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "        self._val_celltype = ['A549']\n",
    "        self._test_celltype = ['GM12878']\n",
    "        self._all_chroms = self._train_chroms + self._test_chroms\n",
    "        self._all_celltypes = self._train_celltypes + self._val_celltype + self._test_celltype\n",
    "        \n",
    "        self._metadata_map = {}\n",
    "        self._metadata_map['chr'] = self._all_chroms\n",
    "        self._metadata_map['celltype'] = self._all_celltypes\n",
    "        \n",
    "        # Get the splits\n",
    "        if split_scheme=='official':\n",
    "            split_scheme = 'standard'\n",
    "        \n",
    "        self._split_scheme = split_scheme\n",
    "        self._split_dict = {\n",
    "            'train': 0,\n",
    "            'id_val': 1,\n",
    "            'test': 2,\n",
    "            'val': 3\n",
    "        }\n",
    "        self._split_names = {\n",
    "            'train': 'Train',\n",
    "            'id_val': 'Validation (ID)',\n",
    "            'test': 'Test',\n",
    "            'val': 'Validation (OOD)',\n",
    "        }\n",
    "        \n",
    "        # Load sequence and DNase features\n",
    "        sequence_filename = os.path.join(self._data_dir, 'sequence.npz')\n",
    "        seq_arr = np.load(sequence_filename)\n",
    "        self._seq_bp = {}\n",
    "        for chrom in self._all_chroms: #seq_arr:\n",
    "            self._seq_bp[chrom] = seq_arr[chrom]\n",
    "            print(chrom, time.time() - itime)\n",
    "        \n",
    "        self._dnase_allcelltypes = {}\n",
    "        for ct in self._all_celltypes:\n",
    "            \"\"\"\n",
    "            dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "            dnase_npz_contents = np.load(dnase_filename)\n",
    "            self._dnase_allcelltypes[ct] = {}\n",
    "            for chrom in self._all_chroms: #self._seq_bp:\n",
    "                self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "            \"\"\"\n",
    "            self._dnase_allcelltypes[ct] = os.path.join(self._data_dir, 'DNASE.{}.fc.signal.bigwig'.format(ct))\n",
    "            print(ct, time.time() - itime)\n",
    "        \n",
    "        # Read in metadata dataframe from training+validation data\n",
    "        train_regions_labeled = pd.read_csv(os.path.join(self._data_dir, 'labels/{}.train.labels.tsv.gz'.format(self._transcription_factor)), sep='\\t')\n",
    "        val_regions_labeled = pd.read_csv(os.path.join(self._data_dir, 'labels/{}.val.labels.tsv.gz'.format(self._transcription_factor)), sep='\\t')\n",
    "        training_df = train_regions_labeled[np.isin(train_regions_labeled['chr'], self._train_chroms)]\n",
    "        val_df = val_regions_labeled[np.isin(val_regions_labeled['chr'], self._test_chroms)]\n",
    "        all_df = pd.concat([training_df, val_df])\n",
    "        \n",
    "        # Get the y values, and remove ambiguous labels by default.\n",
    "        pd_list = []\n",
    "        for ct in _all_celltypes:\n",
    "            tc_chr = all_df[['chr', 'start', 'stop', ct]]\n",
    "            tc_chr.columns = ['chr', 'start', 'stop', 'y']\n",
    "            y_array = tc_chr['y'].replace({'U': 0, 'B': 1, 'A': -1}).values\n",
    "\n",
    "            # Now filter out ambiguous labels\n",
    "            non_ambig_mask = (y_array != -1)\n",
    "            tc_chr['y'] = y_array\n",
    "            tc_chr = tc_chr[non_ambig_mask]\n",
    "\n",
    "            tc_chr.insert(len(tc_chr.columns), 'celltype', ct)\n",
    "            pd_list.append(tc_chr)\n",
    "            print(time.time() - itime)\n",
    "        self._metadata_df = pd.concat(pd_list)\n",
    "        \n",
    "        # Downsample negatives to balance each celltype\n",
    "        samp_ndces = []\n",
    "        itime = time.time()\n",
    "        neg_msk = (self._metadata_df['y'] == 0)\n",
    "        pos_msk = (self._metadata_df['y'] == 1)\n",
    "        for ct in _all_celltypes:\n",
    "            celltype_msk = (self._metadata_df['celltype'] == ct)\n",
    "            print(ct, time.time() - itime)\n",
    "            neg_ct_msk = np.logical_and(celltype_msk, neg_msk)\n",
    "            pos_ct_msk = np.logical_and(celltype_msk, pos_msk)\n",
    "            print(ct, time.time() - itime)\n",
    "            neg_ndces = np.where(neg_ct_msk)[0]\n",
    "            pos_ndces = np.where(pos_ct_msk)[0]\n",
    "            np.random.seed(42)\n",
    "            samp_neg_ndces = np.random.choice(neg_ndces, size=len(pos_ndces), replace=False)\n",
    "            samp_ndces.extend(samp_neg_ndces)\n",
    "            samp_ndces.extend(pos_ndces)\n",
    "            print(ct, time.time() - itime)\n",
    "        self._metadata_df = self._metadata_df.iloc[samp_ndces, :]\n",
    "        \n",
    "        train_regions_mask = np.isin(self._metadata_df['chr'], self._train_chroms)\n",
    "        val_regions_mask = np.isin(self._metadata_df['chr'], self._test_chroms)\n",
    "        train_celltype_mask = np.isin(self._metadata_df['celltype'], self._train_celltypes)\n",
    "        val_celltype_mask = np.isin(self._metadata_df['celltype'], self._val_celltype)\n",
    "        test_celltype_mask = np.isin(self._metadata_df['celltype'], self._test_celltype)\n",
    "        \n",
    "        split_array = -1*np.ones(self._metadata_df.shape[0]).astype(int)\n",
    "        split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = self._split_dict['train']\n",
    "        split_array[np.logical_and(val_regions_mask, test_celltype_mask)] = self._split_dict['test']\n",
    "        # Validate using test chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "        split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = self._split_dict['val']\n",
    "        split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = self._split_dict['id_val']\n",
    "        \n",
    "        if self._split_scheme=='standard':\n",
    "            self._metadata_df.insert(len(self._metadata_df.columns), 'split', split_array)\n",
    "        else:\n",
    "            raise ValueError(f'Split scheme {self._split_scheme} not recognized')\n",
    "        \n",
    "        self._metadata_df = self._metadata_df[self._metadata_df['split'] != -1]\n",
    "        self._split_array = self._metadata_df['split'].values\n",
    "        \n",
    "        chr_ints = self._metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['chr'])] )).values\n",
    "        celltype_ints = self._metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['celltype'])] )).values\n",
    "        self._y_array = torch.LongTensor(np.array(self._metadata_df['y']))\n",
    "        \n",
    "        self._metadata_array = torch.stack(\n",
    "            (torch.LongTensor(chr_ints), \n",
    "             torch.LongTensor(celltype_ints), \n",
    "             self._y_array),\n",
    "            dim=1)\n",
    "        self._metadata_fields = ['chr', 'celltype', 'y']\n",
    "        \n",
    "        self._eval_grouper = CombinatorialGrouper(\n",
    "            dataset=self,\n",
    "            groupby_fields=['celltype'])\n",
    "        \n",
    "        self._metric = Accuracy()\n",
    "        \n",
    "        super().__init__(root_dir, download, split_scheme)\n",
    "\n",
    "    def get_input(self, idx):\n",
    "        \"\"\"\n",
    "        Returns x for a given idx in metadata_array, which has been filtered to only take windows with the desired stride.\n",
    "        Computes this from: \n",
    "        (1) sequence features in self._seq_bp\n",
    "        (2) DNase bigwig file paths in self._dnase_allcelltypes\n",
    "        (3) Metadata for the index (location along the genome with 200bp window width)\n",
    "        \"\"\"\n",
    "        \n",
    "        this_metadata = self._metadata_df.iloc[idx, :]\n",
    "        \"\"\"\n",
    "        flank_size = 400\n",
    "        interval_start = this_metadata['start'] - flank_size\n",
    "        interval_end = this_metadata['stop'] + flank_size\n",
    "        dnase_this = self._dnase_allcelltypes[this_metadata['celltype']][this_metadata['chr']][interval_start:interval_end]\n",
    "        seq_this = self._seq_bp[this_metadata['chr']][interval_start:interval_end]\n",
    "        return torch.tensor(np.column_stack([seq_this, dnase_this]))\n",
    "        \"\"\"\n",
    "        window_size = 12800\n",
    "        interval_start = this_metadata['start']\n",
    "        interval_end = this_metadata['stop'] + window_size\n",
    "        seq_this = self._seq_bp[this_metadata['chr']][interval_start:interval_end]\n",
    "        dnase_bw = self._dnase_allcelltypes[this_metadata['celltype']]\n",
    "        dnase_this = dnase_bw.values(chrom, interval_start, interval_end, numpy=True)\n",
    "        return torch.tensor(np.column_stack([seq_this, dnase_this]))\n",
    "        \n",
    "    def eval(self, y_pred, y_true, metadata):\n",
    "        return self.standard_group_eval(\n",
    "            self._metric,\n",
    "            self._eval_grouper,\n",
    "            y_pred, y_true, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 3.962329387664795\n",
      "chr9 6.259538888931274\n",
      "chr11 8.446826934814453\n",
      "chr1 12.49940538406372\n",
      "chr8 14.91869592666626\n",
      "chr21 15.700694799423218\n",
      "H1-hESC 23.95099449157715\n",
      "HCT116 31.26502823829651\n",
      "HeLa-S3 39.382277488708496\n",
      "HepG2 47.24500226974487\n",
      "K562 55.079211711883545\n",
      "A549 62.405343532562256\n",
      "GM12878 70.00356984138489\n",
      "H1-hESC 8.160386562347412\n",
      "H1-hESC 8.546203374862671\n",
      "H1-hESC 9.868412971496582\n",
      "HCT116 17.121587991714478\n",
      "HCT116 17.524660110473633\n",
      "HCT116 18.90956425666809\n",
      "HeLa-S3 26.98938488960266\n",
      "HeLa-S3 27.376858234405518\n",
      "HeLa-S3 28.7989022731781\n",
      "HepG2 36.29348182678223\n",
      "HepG2 36.668752908706665\n",
      "HepG2 38.151512145996094\n",
      "K562 45.96789216995239\n",
      "K562 46.33995985984802\n",
      "K562 47.87280249595642\n",
      "A549 55.380892276763916\n",
      "A549 55.75924301147461\n",
      "A549 57.22686314582825\n",
      "GM12878 65.09361720085144\n",
      "GM12878 65.50619888305664\n",
      "GM12878 66.9196424484253\n"
     ]
    }
   ],
   "source": [
    "full_dataset_encode = EncodeTFBSDataset(\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyBigWig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-26e8a80fbe25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# full_dataset_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyBigWig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyBigWig'"
     ]
    }
   ],
   "source": [
    "# full_dataset_encode\n",
    "import pyBigWig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice(1210796, size=128)\n",
    "seta = [full_dataset_encode.get_input(x) for x in a]\n",
    "seta[0].shape\n",
    "\n",
    "# full_dataset = copy.deepcopy(full_dataset_encode)\n",
    "# full_dataset = copy.deepcopy(full_dataset_camelyon17)\n",
    "# full_dataset_camelyon17.split_dict\n",
    "\n",
    "# full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data...\n",
      "    hospital = 0: n = 53425\n",
      "    hospital = 1: n = 0\n",
      "    hospital = 2: n = 0\n",
      "    hospital = 3: n = 116959\n",
      "    hospital = 4: n = 132052\n",
      "Validation (ID) data...\n",
      "    hospital = 0: n = 6011\n",
      "    hospital = 1: n = 0\n",
      "    hospital = 2: n = 0\n",
      "    hospital = 3: n = 12879\n",
      "    hospital = 4: n = 14670\n",
      "Test data...\n",
      "    hospital = 0: n = 0\n",
      "    hospital = 1: n = 0\n",
      "    hospital = 2: n = 85054\n",
      "    hospital = 3: n = 0\n",
      "    hospital = 4: n = 0\n",
      "Validation (OOD) data...\n",
      "    hospital = 0: n = 0\n",
      "    hospital = 1: n = 34904\n",
      "    hospital = 2: n = 0\n",
      "    hospital = 3: n = 0\n",
      "    hospital = 4: n = 0\n",
      "Dout: 2\n"
     ]
    }
   ],
   "source": [
    "config = config_camelyon\n",
    "\n",
    "\n",
    "train_grouper = CombinatorialGrouper(\n",
    "    dataset=full_dataset,\n",
    "    groupby_fields=config.groupby_fields)\n",
    "\n",
    "datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "    # Loggers\n",
    "    # Loggers\n",
    "    datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    if config.use_wandb:\n",
    "        initialize_wandb(config)\n",
    "\n",
    "# Logging dataset info\n",
    "if config.no_group_logging and full_dataset.is_classification and full_dataset.y_size==1:\n",
    "    log_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=['y'])\n",
    "elif config.no_group_logging:\n",
    "    log_grouper = None\n",
    "else:\n",
    "    log_grouper = train_grouper\n",
    "log_group_data(datasets, log_grouper, logger)\n",
    "\n",
    "## Initialize algorithm\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=datasets,\n",
    "    train_grouper=train_grouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wilds.datasets.camelyon17_dataset.Camelyon17Dataset at 0x7f99b84f3e80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algorithm.device\n",
    "full_dataset\n",
    "# datasets['train']['loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in datasets['train']['loader']:\n",
    "    x, y_true, metadata = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1406, -0.0628],\n",
       "        [ 0.0534,  0.0359],\n",
       "        [-0.0174, -0.0097],\n",
       "        [-0.0571, -0.2381],\n",
       "        [ 0.1590, -0.0559],\n",
       "        [ 0.1254, -0.0139],\n",
       "        [-0.0423,  0.0439],\n",
       "        [ 0.1621,  0.0730],\n",
       "        [ 0.0554,  0.0796],\n",
       "        [-0.0532,  0.0667],\n",
       "        [-0.1927, -0.0387],\n",
       "        [ 0.1352, -0.0385],\n",
       "        [-0.1320,  0.0140],\n",
       "        [-0.0531, -0.1171],\n",
       "        [-0.0378, -0.0134],\n",
       "        [ 0.1047,  0.0298],\n",
       "        [ 0.0355, -0.0497],\n",
       "        [ 0.1065, -0.0218],\n",
       "        [-0.1883,  0.1298],\n",
       "        [ 0.0699, -0.0875],\n",
       "        [-0.1233,  0.1793],\n",
       "        [ 0.0151,  0.0708],\n",
       "        [-0.0973, -0.0033],\n",
       "        [ 0.1027, -0.2456],\n",
       "        [ 0.0433, -0.0441],\n",
       "        [ 0.1013, -0.1020],\n",
       "        [ 0.1309, -0.0051],\n",
       "        [ 0.0028, -0.0558],\n",
       "        [ 0.0635,  0.0575],\n",
       "        [-0.0066,  0.0666],\n",
       "        [-0.0076, -0.0375],\n",
       "        [ 0.1336,  0.0024]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets['train']['dataset'].size()\n",
    "algorithm.model(x.to(algorithm.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.eval_only:\n",
    "    ## Load saved results if resuming\n",
    "    resume_success = False\n",
    "    if resume:\n",
    "        save_path = os.path.join(config.log_dir, 'last_model.pth')\n",
    "        if not os.path.exists(save_path):\n",
    "            epochs = [\n",
    "                int(file.split('_')[0])\n",
    "                for file in os.listdir(config.log_dir) if file.endswith('.pth')]\n",
    "            if len(epochs) > 0:\n",
    "                latest_epoch = max(epochs)\n",
    "                save_path = os.path.join(config.log_dir, f'{latest_epoch}_model.pth')\n",
    "        try:\n",
    "            prev_epoch, best_val_metric = load(algorithm, save_path)\n",
    "            epoch_offset = prev_epoch + 1\n",
    "            logger.write(f'Resuming from epoch {epoch_offset} with best val metric {best_val_metric}')\n",
    "            resume_success = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    if resume_success == False:\n",
    "        epoch_offset=0\n",
    "        best_val_metric=None\n",
    "\n",
    "\n",
    "    train(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        general_logger=logger,\n",
    "        config=config,\n",
    "        epoch_offset=epoch_offset,\n",
    "        best_val_metric=best_val_metric)\n",
    "else:\n",
    "    if config.eval_epoch is None:\n",
    "        eval_model_path = os.path.join(config.log_dir, 'best_model.pth')\n",
    "    else:\n",
    "        eval_model_path = os.path.join(config.log_dir, f'{config.eval_epoch}_model.pth')\n",
    "    best_epoch, best_val_metric = load(algorithm, eval_model_path)\n",
    "    if config.eval_epoch is None:\n",
    "        epoch = best_epoch\n",
    "    else:\n",
    "        epoch = config.eval_epoch\n",
    "    evaluate(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        epoch=epoch,\n",
    "        general_logger=logger,\n",
    "        config=config)\n",
    "\n",
    "logger.close()\n",
    "for split in datasets:\n",
    "    datasets[split]['eval_logger'].close()\n",
    "    datasets[split]['algo_logger'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wilds.datasets.camelyon17_dataset.Camelyon17Dataset at 0x7f0332cdd520>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in full_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Beagle(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural net models over genomic sequence.\n",
    "    Input:\n",
    "        - sequence_length: int (default 1000) \n",
    "        - Shape: (N, 5, sequence_length, 1) with batch size N.\n",
    "    \n",
    "    Output:\n",
    "        - prediction (Tensor): float torch tensor of shape (N, )\n",
    "    \n",
    "    TODO: Finish docstring.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int\n",
    "        n_genomic_features : int\n",
    "        \"\"\"\n",
    "        super(Beagle, self).__init__()\n",
    "\n",
    "        self.dropout = 0.3\n",
    "        self.num_cell_types = 1\n",
    "        self.conv1 = nn.Conv2d(5, 50, (19, 1), stride = (1, 1), padding=(9,0))\n",
    "        self.conv2 = nn.Conv2d(50, 50, (11, 1), stride = (1, 1), padding = (5,0))\n",
    "        self.conv3 = nn.Conv2d(50, 50, (7, 1), stride = (1, 1), padding = (4,0))\n",
    "        self.bn1 = nn.BatchNorm2d(50)\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "        self.bn3 = nn.BatchNorm2d(50)\n",
    "        self.maxpool1 = nn.MaxPool2d((3, 1))\n",
    "        self.maxpool2 = nn.MaxPool2d((4, 1))\n",
    "        self.maxpool3 = nn.MaxPool2d((4, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(4200, 1000)\n",
    "        self.bn4 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.bn5 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        self.fc3 = nn.Linear(1000, self.num_cell_types)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = s.permute(0, 2, 1).contiguous()                          # batch_size x 5 x 1000\n",
    "        s = s.view(-1, 5, 1000, 1)                                   # batch_size x 5 x 1000 x 1 [5 channels]\n",
    "        s = self.maxpool1(F.relu(self.bn1(self.conv1(s))))           # batch_size x 300 x 333 x 1\n",
    "        s = self.maxpool2(F.relu(self.bn2(self.conv2(s))))           # batch_size x 200 x 83 x 1\n",
    "        s = self.maxpool3(F.relu(self.bn3(self.conv3(s))))           # batch_size x 200 x 21 x 1\n",
    "        s = s.view(-1, 4200)\n",
    "        conv_out = s\n",
    "\n",
    "        s = F.dropout(F.relu(self.bn4(self.fc1(s))), p=self.dropout, training=self.training)  # batch_size x 1000\n",
    "        s = F.dropout(F.relu(self.bn5(self.fc2(s))), p=self.dropout, training=self.training)  # batch_size x 1000\n",
    "        \n",
    "        s = self.fc3(s)\n",
    "\n",
    "        return s, conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, 7, padding=2), \n",
    "        nn.BatchNorm1d(out_channels), \n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(out_channels, out_channels, 7, padding=3), \n",
    "        nn.BatchNorm1d(out_channels), \n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dconv_down1 = double_conv(6, 15)\n",
    "        self.dconv_down2 = double_conv(15, 22)\n",
    "        self.dconv_down3 = double_conv(22, 33)\n",
    "        self.dconv_down4 = double_conv(33, 49)\n",
    "        self.dconv_down5 = double_conv(49, 73)\n",
    "        self.dconv_down6 = double_conv(73, 109)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up5 = double_conv(73 + 109, 73)\n",
    "        self.dconv_up4 = double_conv(49 + 73, 49)\n",
    "        self.dconv_up3 = double_conv(33 + 49, 33)\n",
    "        self.dconv_up2 = double_conv(22 + 33, 22)\n",
    "        self.dconv_up1 = double_conv(15 + 22, 15)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(15, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        \n",
    "        conv4 = self.dconv_down4(x)\n",
    "        x = self.maxpool(conv4)\n",
    "        \n",
    "        conv5 = self.dconv_down5(x)\n",
    "        x = self.maxpool(conv5)\n",
    "        \n",
    "        x = self.dconv_down6(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv5], dim=1)\n",
    "        \n",
    "        x = self.dconv_up5(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        \n",
    "        x = self.dconv_up4(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (dconv_down1): Sequential(\n",
       "    (0): Conv1d(6, 15, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(15, 15, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_down2): Sequential(\n",
       "    (0): Conv1d(15, 22, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(22, 22, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_down3): Sequential(\n",
       "    (0): Conv1d(22, 33, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(33, 33, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_down4): Sequential(\n",
       "    (0): Conv1d(33, 49, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(49, 49, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_down5): Sequential(\n",
       "    (0): Conv1d(49, 73, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(73, 73, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_down6): Sequential(\n",
       "    (0): Conv1d(73, 109, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(109, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(109, 109, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(109, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (dconv_up5): Sequential(\n",
       "    (0): Conv1d(182, 73, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(73, 73, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_up4): Sequential(\n",
       "    (0): Conv1d(122, 49, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(49, 49, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_up3): Sequential(\n",
       "    (0): Conv1d(82, 33, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(33, 33, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_up2): Sequential(\n",
       "    (0): Conv1d(55, 22, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(22, 22, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (dconv_up1): Sequential(\n",
       "    (0): Conv1d(37, 15, kernel_size=(7,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(15, 15, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (4): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_last): Conv2d(15, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485773"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "lst = [(x[0], x[1].numel()) for x in model.named_parameters()]\n",
    "#np.sum([x[1] for x in lst])\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6955906"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(algorithm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(x[0], x[1].numel()) for x in algorithm.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
