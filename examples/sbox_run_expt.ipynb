{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_expt.py contents\n",
    "\n",
    "## 1) Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7360ca071eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psutil'"
     ]
    }
   ],
   "source": [
    "import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool\n",
    "from train import train, evaluate\n",
    "from algorithms.initializer import initialize_algorithm\n",
    "from transforms import initialize_transform\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs='?', const=True, default=False, type=<function parse_bool at 0x7f7204547040>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' set default hyperparams in default_hyperparams.py '''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=supported.datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--train_transform', choices=supported.transforms)\n",
    "parser.add_argument('--eval_transform', choices=supported.transforms)\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='target resolution. for example --target_resolution 224 224 for standard resnet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices = supported.losses)\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--algo_log_metric')\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int)\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "config_camelyon = populate_defaults(config_camelyon)\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode = populate_defaults(config_encode)\n",
    "\n",
    "config = config_camelyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: camelyon17\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Loader kwargs: {'num_workers': 4, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 2\n",
      "Batch size: 32\n",
      "Eval loader: standard\n",
      "Model: densenet121\n",
      "Model kwargs: {'pretrained': False}\n",
      "Train transform: image_base\n",
      "Eval transform: image_base\n",
      "Target resolution: (224, 224)\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Loss function: cross_entropy\n",
      "Groupby fields: ['hospital']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: 0.1\n",
      "Irm lambda: 1.0\n",
      "Irm penalty anneal iters: None\n",
      "Algo log metric: accuracy\n",
      "Val metric: acc_avg\n",
      "Val metric decreasing: False\n",
      "N epochs: 5\n",
      "Optimizer: SGD\n",
      "Lr: 0.001\n",
      "Weight decay: 0.01\n",
      "Max grad norm: None\n",
      "Optimizer kwargs: {'momentum': 0.9}\n",
      "Scheduler: None\n",
      "Scheduler kwargs: {}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: cuda:0\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "No group logging: False\n",
      "Use wandb: False\n",
      "Progress bar: False\n",
      "Resume: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "config.device = torch.device(\"cuda:\" + str(config.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "## Initialize logs\n",
    "if os.path.exists(config.log_dir) and config.resume:\n",
    "    resume=True\n",
    "    mode='a'\n",
    "elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "    resume=False\n",
    "    mode='a'\n",
    "else:\n",
    "    resume=False\n",
    "    mode='w'\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "\n",
    "# Record config\n",
    "log_config(config, logger)\n",
    "\n",
    "# Set random seed\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Data\n",
    "full_dataset = supported.datasets[config.dataset](\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# To implement data augmentation (i.e., have different transforms\n",
    "# at training time vs. test time), modify these two lines:\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.train_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.eval_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "full_dataset_camelyon17 = copy.deepcopy(full_dataset)\n",
    "\n",
    "# supported.datasets[config_encode.dataset]\n",
    "# print(config_camelyon.train_transform, config_encode.train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 3.657395362854004\n",
      "chr9 5.770605564117432\n",
      "chr11 7.801896095275879\n",
      "chr1 11.56663990020752\n",
      "chr8 13.764073133468628\n",
      "chr21 14.483267068862915\n",
      "H1-hESC 20.850953817367554\n",
      "HCT116 27.05355429649353\n",
      "HeLa-S3 33.51919412612915\n",
      "HepG2 39.89570116996765\n",
      "K562 46.36982774734497\n",
      "A549 52.82617139816284\n",
      "GM12878 59.167165994644165\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "root_dir='data'\n",
    "download=False\n",
    "split_scheme='official'\n",
    "\n",
    "itime = time.time()\n",
    "_dataset_name = 'encode-tfbs'\n",
    "_version = '1.0'\n",
    "_download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "_data_dir = 'data/encode-tfbs_v1.0'\n",
    "_y_size = 1\n",
    "_n_classes = 2\n",
    "\n",
    "# _train_chroms = ['chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "_train_chroms = ['chr2', 'chr9', 'chr11']\n",
    "_test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "_transcription_factor = 'MAX'\n",
    "_train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "_val_celltype = ['A549']\n",
    "_test_celltype = ['GM12878']\n",
    "_all_chroms = _train_chroms + _test_chroms\n",
    "_all_celltypes = _train_celltypes + _val_celltype + _test_celltype\n",
    "\n",
    "_metadata_map = {}\n",
    "_metadata_map['chr'] = _all_chroms #['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "_metadata_map['celltype'] = _all_celltypes\n",
    "\n",
    "# Get the splits\n",
    "if split_scheme=='official':\n",
    "    split_scheme = 'standard'\n",
    "\n",
    "_split_scheme = split_scheme\n",
    "_split_dict = {\n",
    "    'train': 0,\n",
    "    'id_val': 1,\n",
    "    'test': 2,\n",
    "    'val': 3\n",
    "}\n",
    "_split_names = {\n",
    "    'train': 'Train',\n",
    "    'id_val': 'Validation (ID)',\n",
    "    'test': 'Test',\n",
    "    'val': 'Validation (OOD)',\n",
    "}\n",
    "\n",
    "# Load sequence and DNase features\n",
    "sequence_filename = os.path.join(_data_dir, 'sequence.npz')\n",
    "seq_arr = np.load(sequence_filename)\n",
    "_seq_bp = {}\n",
    "for chrom in _all_chroms: #seq_arr:\n",
    "    _seq_bp[chrom] = seq_arr[chrom]\n",
    "    print(chrom, time.time() - itime)\n",
    "\n",
    "_dnase_allcelltypes = {}\n",
    "for ct in _all_celltypes:\n",
    "    dnase_filename = os.path.join(_data_dir, '{}_dnase.npz'.format(ct))\n",
    "    dnase_npz_contents = np.load(dnase_filename)\n",
    "    _dnase_allcelltypes[ct] = {}\n",
    "    for chrom in _all_chroms: #_seq_bp:\n",
    "        _dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "    print(ct, time.time() - itime)\n",
    "\n",
    "# Read in metadata dataframe from training+validation data\n",
    "train_regions_labeled = pd.read_csv(os.path.join(_data_dir, 'labels/{}.train.labels.tsv.gz'.format(_transcription_factor)), sep='\\t')\n",
    "val_regions_labeled = pd.read_csv(os.path.join(_data_dir, 'labels/{}.val.labels.tsv.gz'.format(_transcription_factor)), sep='\\t')\n",
    "training_df = train_regions_labeled[np.isin(train_regions_labeled['chr'], _train_chroms)]\n",
    "val_df = val_regions_labeled[np.isin(val_regions_labeled['chr'], _test_chroms)]\n",
    "all_df = pd.concat([training_df, val_df])\n",
    "\n",
    "# Filter by start/stop coordinate if needed (TODO: remove for final version)\n",
    "# filter_msk = all_df['start'] >= 0\n",
    "# filter_msk = all_df['start']%1000 == 0\n",
    "# all_df = all_df[filter_msk]\n",
    "\n",
    "pd_list = []\n",
    "for ct in _all_celltypes:\n",
    "    tc_chr = all_df[['chr', 'start', 'stop', ct]]\n",
    "    tc_chr.columns = ['chr', 'start', 'stop', 'y']\n",
    "    tc_chr.insert(len(tc_chr.columns), 'celltype', ct)\n",
    "    pd_list.append(tc_chr)\n",
    "metadata_df = pd.concat(pd_list)\n",
    "\n",
    "# Get the y values, and remove ambiguous labels by default.\n",
    "y_array = metadata_df['y'].replace({'U': 0, 'B': 1, 'A': -1}).values\n",
    "non_ambig_mask = (y_array != -1)\n",
    "metadata_df['y'] = y_array\n",
    "_metadata_df = metadata_df[non_ambig_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "samp_ndces = []\n",
    "itime = time.time()\n",
    "for ct in _all_celltypes:\n",
    "    neg_msk = np.logical_and((_metadata_df['celltype'] == ct), (_metadata_df['y'] == 0))\n",
    "    pos_msk = np.logical_and((_metadata_df['celltype'] == ct), (_metadata_df['y'] == 1))\n",
    "    neg_ndces = np.where(neg_msk)[0]\n",
    "    pos_ndces = np.where(pos_msk)[0]\n",
    "    np.random.seed(42)\n",
    "    samp_neg_ndces = np.random.choice(neg_ndces, size=len(pos_ndces), replace=False)\n",
    "    samp_ndces.extend(samp_neg_ndces)\n",
    "    samp_ndces.extend(pos_ndces)\n",
    "    print(ct, time.time() - itime)\n",
    "\n",
    "_metadata_df = _metadata_df.iloc[samp_ndces, :]\n",
    "\n",
    "train_regions_mask = np.isin(_metadata_df['chr'], _train_chroms)\n",
    "val_regions_mask = np.isin(_metadata_df['chr'], _test_chroms)\n",
    "train_celltype_mask = np.isin(_metadata_df['celltype'], _train_celltypes)\n",
    "val_celltype_mask = np.isin(_metadata_df['celltype'], _val_celltype)\n",
    "test_celltype_mask = np.isin(_metadata_df['celltype'], _test_celltype)\n",
    "\n",
    "split_array = -1*np.ones(_metadata_df.shape[0]).astype(int)\n",
    "split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = _split_dict['train']\n",
    "split_array[np.logical_and(val_regions_mask, test_celltype_mask)] = _split_dict['test']\n",
    "# Validate using test chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = _split_dict['val']\n",
    "split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = _split_dict['id_val']\n",
    "\n",
    "if _split_scheme=='standard':\n",
    "    _metadata_df.insert(len(_metadata_df.columns), 'split', split_array)\n",
    "else:\n",
    "    raise ValueError(f'Split scheme {_split_scheme} not recognized')\n",
    "\n",
    "_metadata_df = _metadata_df[_metadata_df['split'] != -1]\n",
    "_split_array = _metadata_df['split'].values\n",
    "\n",
    "chr_ints = _metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['chr'])] )).values\n",
    "celltype_ints = _metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['celltype'])] )).values\n",
    "_y_array = torch.LongTensor(np.array(_metadata_df['y']))\n",
    "\n",
    "_metadata_array = torch.stack(\n",
    "    (torch.LongTensor(chr_ints), \n",
    "     torch.LongTensor(celltype_ints), \n",
    "     _y_array),\n",
    "    dim=1)\n",
    "_metadata_fields = ['chr', 'celltype', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "class EncodeTFBSDataset(WILDSDataset):\n",
    "    \"\"\"\n",
    "    ENCODE-DREAM-wilds dataset of transcription factor binding sites. \n",
    "    This is a subset of the dataset from the ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge. \n",
    "    \n",
    "    Input (x):\n",
    "        1000-base-pair regions of sequence with a quantified chromatin accessibility readout.\n",
    "\n",
    "    Label (y):\n",
    "        y is binary. It is 1 if the central 200bp region is bound by the transcription factor MAX, and 0 otherwise.\n",
    "\n",
    "    Metadata:\n",
    "        Each sequence is annotated with the celltype of origin (a string) and the chromosome of origin (a string).\n",
    "    \n",
    "    Website:\n",
    "        https://www.synapse.org/#!Synapse:syn6131484\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir='data', download=False, split_scheme='official'):\n",
    "        itime = time.time()\n",
    "        self._dataset_name = 'encode-tfbs'\n",
    "        self._version = '1.0'\n",
    "        self._download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "        self._data_dir = self.initialize_data_dir(root_dir, download)\n",
    "        self._y_size = 1\n",
    "        self._n_classes = 2\n",
    "        \n",
    "        self._train_chroms = ['chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "        # self._train_chroms = ['chr2', 'chr9', 'chr11']\n",
    "        self._test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "        self._transcription_factor = 'MAX'\n",
    "        self._train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "        self._val_celltype = ['A549']\n",
    "        self._test_celltype = ['GM12878']\n",
    "        self._all_chroms = self._train_chroms + self._test_chroms\n",
    "        self._all_celltypes = self._train_celltypes + self._val_celltype + self._test_celltype\n",
    "        \n",
    "        self._metadata_map = {}\n",
    "        self._metadata_map['chr'] = self._all_chroms #['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "        self._metadata_map['celltype'] = self._all_celltypes\n",
    "        \n",
    "        # Get the splits\n",
    "        if split_scheme=='official':\n",
    "            split_scheme = 'standard'\n",
    "        \n",
    "        self._split_scheme = split_scheme\n",
    "        self._split_dict = {\n",
    "            'train': 0,\n",
    "            'id_val': 1,\n",
    "            'test': 2,\n",
    "            'val': 3\n",
    "        }\n",
    "        self._split_names = {\n",
    "            'train': 'Train',\n",
    "            'id_val': 'Validation (ID)',\n",
    "            'test': 'Test',\n",
    "            'val': 'Validation (OOD)',\n",
    "        }\n",
    "        \n",
    "        # Load sequence and DNase features\n",
    "        sequence_filename = os.path.join(self._data_dir, 'sequence.npz')\n",
    "        seq_arr = np.load(sequence_filename)\n",
    "        self._seq_bp = {}\n",
    "        for chrom in self._all_chroms: #seq_arr:\n",
    "            self._seq_bp[chrom] = seq_arr[chrom]\n",
    "            print(chrom, time.time() - itime)\n",
    "        \n",
    "        self._dnase_allcelltypes = {}\n",
    "        for ct in self._all_celltypes:\n",
    "            dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "            dnase_npz_contents = np.load(dnase_filename)\n",
    "            self._dnase_allcelltypes[ct] = {}\n",
    "            for chrom in self._all_chroms: #self._seq_bp:\n",
    "                self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "            print(ct, time.time() - itime)\n",
    "        \n",
    "        # Read in metadata dataframe from training+validation data\n",
    "        train_regions_labeled = pd.read_csv(os.path.join(self._data_dir, 'labels/{}.train.labels.tsv.gz'.format(self._transcription_factor)), sep='\\t')\n",
    "        val_regions_labeled = pd.read_csv(os.path.join(self._data_dir, 'labels/{}.val.labels.tsv.gz'.format(self._transcription_factor)), sep='\\t')\n",
    "        training_df = train_regions_labeled[np.isin(train_regions_labeled['chr'], self._train_chroms)]\n",
    "        val_df = val_regions_labeled[np.isin(val_regions_labeled['chr'], self._test_chroms)]\n",
    "        all_df = pd.concat([training_df, val_df])\n",
    "        \n",
    "        # Filter by start/stop coordinate if needed (TODO: remove for final version)\n",
    "        # filter_msk = all_df['start'] >= 0\n",
    "        # filter_msk = all_df['start']%1000 == 0\n",
    "        # all_df = all_df[filter_msk]\n",
    "        \n",
    "        pd_list = []\n",
    "        for ct in self._all_celltypes:\n",
    "            tc_chr = all_df[['chr', 'start', 'stop', ct]]\n",
    "            tc_chr.columns = ['chr', 'start', 'stop', 'y']\n",
    "            tc_chr.insert(len(tc_chr.columns), 'celltype', ct)\n",
    "            pd_list.append(tc_chr)\n",
    "        metadata_df = pd.concat(pd_list)\n",
    "        \n",
    "        # Get the y values, and remove ambiguous labels by default.\n",
    "        y_array = metadata_df['y'].replace({'U': 0, 'B': 1, 'A': -1}).values\n",
    "        non_ambig_mask = (y_array != -1)\n",
    "        metadata_df['y'] = y_array\n",
    "        self._metadata_df = metadata_df[non_ambig_mask]\n",
    "        \n",
    "        samp_ndces = []\n",
    "        itime = time.time()\n",
    "        for ct in self._all_celltypes:\n",
    "            neg_msk = np.logical_and((self._metadata_df['celltype'] == ct), (self._metadata_df['y'] == 0))\n",
    "            pos_msk = np.logical_and((self._metadata_df['celltype'] == ct), (self._metadata_df['y'] == 1))\n",
    "            neg_ndces = np.where(neg_msk)[0]\n",
    "            pos_ndces = np.where(pos_msk)[0]\n",
    "            np.random.seed(42)\n",
    "            samp_neg_ndces = np.random.choice(neg_ndces, size=len(pos_ndces), replace=False)\n",
    "            samp_ndces.extend(samp_neg_ndces)\n",
    "            samp_ndces.extend(pos_ndces)\n",
    "            print(ct, time.time() - itime)\n",
    "        self._metadata_df = self._metadata_df.iloc[samp_ndces, :]\n",
    "        \n",
    "        train_regions_mask = np.isin(self._metadata_df['chr'], self._train_chroms)\n",
    "        val_regions_mask = np.isin(self._metadata_df['chr'], self._test_chroms)\n",
    "        train_celltype_mask = np.isin(self._metadata_df['celltype'], self._train_celltypes)\n",
    "        val_celltype_mask = np.isin(self._metadata_df['celltype'], self._val_celltype)\n",
    "        test_celltype_mask = np.isin(self._metadata_df['celltype'], self._test_celltype)\n",
    "        \n",
    "        split_array = -1*np.ones(self._metadata_df.shape[0]).astype(int)\n",
    "        split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = self._split_dict['train']\n",
    "        split_array[np.logical_and(val_regions_mask, test_celltype_mask)] = self._split_dict['test']\n",
    "        # Validate using test chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "        split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = self._split_dict['val']\n",
    "        split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = self._split_dict['id_val']\n",
    "        \n",
    "        if self._split_scheme=='standard':\n",
    "            self._metadata_df.insert(len(self._metadata_df.columns), 'split', split_array)\n",
    "        else:\n",
    "            raise ValueError(f'Split scheme {self._split_scheme} not recognized')\n",
    "        \n",
    "        self._metadata_df = self._metadata_df[self._metadata_df['split'] != -1]\n",
    "        self._split_array = self._metadata_df['split'].values\n",
    "        \n",
    "        chr_ints = self._metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['chr'])] )).values\n",
    "        celltype_ints = self._metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['celltype'])] )).values\n",
    "        self._y_array = torch.LongTensor(np.array(self._metadata_df['y']))\n",
    "        \n",
    "        self._metadata_array = torch.stack(\n",
    "            (torch.LongTensor(chr_ints), \n",
    "             torch.LongTensor(celltype_ints), \n",
    "             self._y_array),\n",
    "            dim=1)\n",
    "        self._metadata_fields = ['chr', 'celltype', 'y']\n",
    "        \n",
    "        self._eval_grouper = CombinatorialGrouper(\n",
    "            dataset=self,\n",
    "            groupby_fields=['celltype'])\n",
    "        \n",
    "        self._metric = Accuracy()\n",
    "        \n",
    "        super().__init__(root_dir, download, split_scheme)\n",
    "\n",
    "    def get_input(self, idx):\n",
    "        \"\"\"\n",
    "        Returns x for a given idx.\n",
    "        Computes this from: \n",
    "        (1) sequence features in self._seq_bp\n",
    "        (2) DNase features in self._dnase_allcelltypes\n",
    "        (3) Metadata for the index (location along the genome with 200bp window width)\n",
    "        \"\"\"\n",
    "        this_metadata = self._metadata_df.iloc[idx, :]\n",
    "        flank_size = 400\n",
    "        interval_start = this_metadata['start'] - flank_size\n",
    "        interval_end = this_metadata['stop'] + flank_size\n",
    "        dnase_this = self._dnase_allcelltypes[this_metadata['celltype']][this_metadata['chr']][interval_start:interval_end]\n",
    "        seq_this = self._seq_bp[this_metadata['chr']][interval_start:interval_end]\n",
    "        return torch.tensor(np.column_stack([seq_this, dnase_this]))\n",
    "\n",
    "    def eval(self, y_pred, y_true, metadata):\n",
    "        return self.standard_group_eval(\n",
    "            self._metric,\n",
    "            self._eval_grouper,\n",
    "            y_pred, y_true, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 3.718320846557617\n",
      "chr3 6.73882269859314\n",
      "chr4 9.651247501373291\n",
      "chr5 12.439628839492798\n",
      "chr6 15.05026388168335\n",
      "chr7 17.475954055786133\n",
      "chr9 19.6206693649292\n",
      "chr10 21.68758535385132\n",
      "chr11 23.74817419052124\n",
      "chr12 25.81403160095215\n",
      "chr13 27.559557676315308\n",
      "chr14 29.18643832206726\n",
      "chr15 30.739391565322876\n",
      "chr16 32.11144256591797\n",
      "chr17 33.348127126693726\n",
      "chr18 34.53834342956543\n",
      "chr19 35.434733629226685\n",
      "chr20 36.399296283721924\n",
      "chr22 37.1924102306366\n",
      "chrX 39.56284308433533\n",
      "chr1 43.3526566028595\n",
      "chr8 45.583492040634155\n",
      "chr21 46.311339378356934\n",
      "H1-hESC 66.45292735099792\n",
      "HCT116 86.06067085266113\n",
      "HeLa-S3 106.47142815589905\n",
      "HepG2 126.59437656402588\n",
      "K562 146.93650436401367\n",
      "A549 167.19306707382202\n",
      "GM12878 187.4349775314331\n"
     ]
    }
   ],
   "source": [
    "full_dataset_encode = EncodeTFBSDataset(\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A549', 'GM12878', 'H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562'],\n",
       "       dtype=object),\n",
       " array([ 5118,  1702,  8460, 12806,  8348, 11774, 12518]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(full_dataset_encode._metadata_df['celltype'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 1, 1, 1]) torch.Size([60726])\n",
      "(array([0, 1]), array([30426, 30300]))\n",
      "(array([0, 1, 2, 3]), array([28556, 25350,  1702,  5118]))\n"
     ]
    }
   ],
   "source": [
    "full_dataset = copy.deepcopy(full_dataset_encode)\n",
    "print(full_dataset._y_array, full_dataset._y_array.shape)\n",
    "print(np.unique(full_dataset.y_array.numpy(), return_counts=True))\n",
    "print(np.unique(full_dataset._metadata_df['split'], return_counts=True))\n",
    "\n",
    "#full_dataset._input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0]) torch.Size([5568233])\n",
      "(array([0, 1]), array([5537933,   30300]))\n",
      "(array([0, 1, 2, 3]), array([2533595, 2163528,  437124,  433986]))\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset._y_array, full_dataset._y_array.shape)\n",
    "print(np.unique(full_dataset.y_array.numpy(), return_counts=True))\n",
    "print(np.unique(full_dataset._metadata_df['split'], return_counts=True))\n",
    "\n",
    "#full_dataset._input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(os.path.join('data/camelyon17_v1.0/metadata.csv'), index_col=0, dtype={'patient': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.metadata_fields\n",
    "config = config_encode\n",
    "#config_encode.groupby_fields\n",
    "\n",
    "train_grouper = CombinatorialGrouper(\n",
    "    dataset=full_dataset,\n",
    "    groupby_fields=config.groupby_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_encode.eval_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data...\n",
      "    celltype = H1-hESC: n = 507309\n",
      "    celltype = HCT116: n = 506458\n",
      "    celltype = HeLa-S3: n = 509974\n",
      "    celltype = HepG2: n = 503007\n",
      "    celltype = K562: n = 506847\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Validation (ID) data...\n",
      "    celltype = H1-hESC: n = 433473\n",
      "    celltype = HCT116: n = 431398\n",
      "    celltype = HeLa-S3: n = 435455\n",
      "    celltype = HepG2: n = 433039\n",
      "    celltype = K562: n = 430163\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Test data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 437124\n",
      "Validation (OOD) data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 433986\n",
      "    celltype = GM12878: n = 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model not recognized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-23d93a467e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m## Initialize algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m algorithm = initialize_algorithm(\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/initializer.py\u001b[0m in \u001b[0;36minitialize_algorithm\u001b[0;34m(config, datasets, train_grouper)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'ERM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         algorithm = ERM(\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0md_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/ERM.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, d_out, grouper, loss, metric, n_train_steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     def __init__(self, config, d_out, grouper, loss,\n\u001b[1;32m      7\u001b[0m             metric, n_train_steps):\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# initialize module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         super().__init__(\n",
      "\u001b[0;32m~/wilds/examples/models/initializer.py\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(config, d_out)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGINVirtual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model not recognized.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Model not recognized."
     ]
    }
   ],
   "source": [
    "datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "    # Loggers\n",
    "    # Loggers\n",
    "    datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    if config.use_wandb:\n",
    "        initialize_wandb(config)\n",
    "\n",
    "# Logging dataset info\n",
    "if config.no_group_logging and full_dataset.is_classification and full_dataset.y_size==1:\n",
    "    log_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=['y'])\n",
    "elif config.no_group_logging:\n",
    "    log_grouper = None\n",
    "else:\n",
    "    log_grouper = train_grouper\n",
    "log_group_data(datasets, log_grouper, logger)\n",
    "\n",
    "## Initialize algorithm\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=datasets,\n",
    "    train_grouper=train_grouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.eval_only:\n",
    "    ## Load saved results if resuming\n",
    "    resume_success = False\n",
    "    if resume:\n",
    "        save_path = os.path.join(config.log_dir, 'last_model.pth')\n",
    "        if not os.path.exists(save_path):\n",
    "            epochs = [\n",
    "                int(file.split('_')[0])\n",
    "                for file in os.listdir(config.log_dir) if file.endswith('.pth')]\n",
    "            if len(epochs) > 0:\n",
    "                latest_epoch = max(epochs)\n",
    "                save_path = os.path.join(config.log_dir, f'{latest_epoch}_model.pth')\n",
    "        try:\n",
    "            prev_epoch, best_val_metric = load(algorithm, save_path)\n",
    "            epoch_offset = prev_epoch + 1\n",
    "            logger.write(f'Resuming from epoch {epoch_offset} with best val metric {best_val_metric}')\n",
    "            resume_success = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    if resume_success == False:\n",
    "        epoch_offset=0\n",
    "        best_val_metric=None\n",
    "\n",
    "\n",
    "    train(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        general_logger=logger,\n",
    "        config=config,\n",
    "        epoch_offset=epoch_offset,\n",
    "        best_val_metric=best_val_metric)\n",
    "else:\n",
    "    if config.eval_epoch is None:\n",
    "        eval_model_path = os.path.join(config.log_dir, 'best_model.pth')\n",
    "    else:\n",
    "        eval_model_path = os.path.join(config.log_dir, f'{config.eval_epoch}_model.pth')\n",
    "    best_epoch, best_val_metric = load(algorithm, eval_model_path)\n",
    "    if config.eval_epoch is None:\n",
    "        epoch = best_epoch\n",
    "    else:\n",
    "        epoch = config.eval_epoch\n",
    "    evaluate(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        epoch=epoch,\n",
    "        general_logger=logger,\n",
    "        config=config)\n",
    "\n",
    "logger.close()\n",
    "for split in datasets:\n",
    "    datasets[split]['eval_logger'].close()\n",
    "    datasets[split]['algo_logger'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Beagle(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural net models over genomic sequence.\n",
    "    Input:\n",
    "        - sequence_length: int (default 1000) \n",
    "        - Shape: (N, 5, sequence_length, 1) with batch size N.\n",
    "    \n",
    "    Output:\n",
    "        - prediction (Tensor): float torch tensor of shape (N, )\n",
    "    \n",
    "    TODO: Finish docstring.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int\n",
    "        n_genomic_features : int\n",
    "        \"\"\"\n",
    "        super(Beagle, self).__init__()\n",
    "\n",
    "        self.dropout = 0.3\n",
    "        self.num_cell_types = 1\n",
    "        self.conv1 = nn.Conv2d(5, 300, (19, 1), stride = (1, 1), padding=(9,0))\n",
    "        self.conv2 = nn.Conv2d(300, 200, (11, 1), stride = (1, 1), padding = (5,0))\n",
    "        self.conv3 = nn.Conv2d(200, 200, (7, 1), stride = (1, 1), padding = (4,0))\n",
    "        self.bn1 = nn.BatchNorm2d(300)\n",
    "        self.bn2 = nn.BatchNorm2d(200)\n",
    "        self.bn3 = nn.BatchNorm2d(200)\n",
    "        self.maxpool1 = nn.MaxPool2d((3, 1))\n",
    "        self.maxpool2 = nn.MaxPool2d((4, 1))\n",
    "        self.maxpool3 = nn.MaxPool2d((4, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(4200, 1000)\n",
    "        self.bn4 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.bn5 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        self.fc3 = nn.Linear(1000, self.num_cell_types)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = s.permute(0, 2, 1).contiguous()                          # batch_size x 5 x 1000\n",
    "        s = s.view(-1, 5, 1000, 1)                                   # batch_size x 5 x 1000 x 1 [5 channels]\n",
    "        s = self.maxpool1(F.relu(self.bn1(self.conv1(s))))           # batch_size x 300 x 333 x 1\n",
    "        s = self.maxpool2(F.relu(self.bn2(self.conv2(s))))           # batch_size x 200 x 83 x 1\n",
    "        s = self.maxpool3(F.relu(self.bn3(self.conv3(s))))           # batch_size x 200 x 21 x 1\n",
    "        s = s.view(-1, 4200)\n",
    "        conv_out = s\n",
    "\n",
    "        s = F.dropout(F.relu(self.bn4(self.fc1(s))), p=self.dropout, training=self.training)  # batch_size x 1000\n",
    "        s = F.dropout(F.relu(self.bn5(self.fc2(s))), p=self.dropout, training=self.training)  # batch_size x 1000\n",
    "        \n",
    "        s = self.fc3(s)\n",
    "\n",
    "        return s, conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nnet.0.weight', 33280),\n",
       " ('nnet.0.bias', 320),\n",
       " ('bdlstm.0.weight_ih_l0', 409600),\n",
       " ('bdlstm.0.weight_hh_l0', 409600),\n",
       " ('bdlstm.0.bias_ih_l0', 1280),\n",
       " ('bdlstm.0.bias_hh_l0', 1280),\n",
       " ('bdlstm.0.weight_ih_l0_reverse', 409600),\n",
       " ('bdlstm.0.weight_hh_l0_reverse', 409600),\n",
       " ('bdlstm.0.bias_ih_l0_reverse', 1280),\n",
       " ('bdlstm.0.bias_hh_l0_reverse', 1280),\n",
       " ('classifier.1.weight', 592000),\n",
       " ('classifier.1.bias', 925),\n",
       " ('classifier.3.weight', 4625),\n",
       " ('classifier.3.bias', 5)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = Beagle2()\n",
    "model = DanQ(50, 5)\n",
    "\n",
    "lst = [(x[0], x[1].numel()) for x in model.named_parameters()]\n",
    "#np.sum([x[1] for x in lst])\n",
    "count_parameters(model)\n",
    "lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
