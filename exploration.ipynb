{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: poverty\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {'no_nl': False, 'fold': 'A', 'use_ood_val': True}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Version: None\n",
      "Unlabeled split: None\n",
      "Unlabeled version: None\n",
      "Use unlabeled y: False\n",
      "Loader kwargs: {'num_workers': 4, 'pin_memory': True}\n",
      "Unlabeled loader kwargs: {'num_workers': 8, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 8\n",
      "Unlabeled n groups per batch: 4\n",
      "Batch size: 32\n",
      "Unlabeled batch size: 64\n",
      "Eval loader: standard\n",
      "Gradient accumulation steps: 1\n",
      "Model: vit_b_16\n",
      "Model kwargs: {'num_channels': 8}\n",
      "Noisystudent add dropout: None\n",
      "Noisystudent dropout rate: None\n",
      "Pretrained model path: None\n",
      "Load featurizer only: False\n",
      "Teacher model path: None\n",
      "Transform: poverty\n",
      "Additional train transform: None\n",
      "Target resolution: None\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Randaugment n: 2\n",
      "Loss function: mse\n",
      "Loss kwargs: {}\n",
      "Groupby fields: ['country']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: 0.1\n",
      "Dann penalty weight: 0.1\n",
      "Dann classifier lr: 0.001\n",
      "Dann featurizer lr: 0.0001\n",
      "Dann discriminator lr: 0.001\n",
      "Afn penalty weight: None\n",
      "Safn delta r: None\n",
      "Hafn r: None\n",
      "Use hafn: False\n",
      "Irm lambda: 1.0\n",
      "Irm penalty anneal iters: None\n",
      "Self training lambda: None\n",
      "Self training threshold: None\n",
      "Pseudolabel t2: None\n",
      "Soft pseudolabels: False\n",
      "Algo log metric: mse\n",
      "Process pseudolabels function: pseudolabel_identity\n",
      "Val metric: r_wg\n",
      "Val metric decreasing: False\n",
      "N epochs: 200\n",
      "Optimizer: Adam\n",
      "Lr: 0.001\n",
      "Weight decay: 0.0\n",
      "Max grad norm: 1.0\n",
      "Optimizer kwargs: {}\n",
      "Scheduler: StepLR\n",
      "Scheduler kwargs: {'gamma': 0.96, 'step_size': 1}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Process outputs function: None\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: cuda\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "Save pred: True\n",
      "No group logging: False\n",
      "Progress bar: False\n",
      "Resume: False\n",
      "Use wandb: False\n",
      "Wandb api key path: None\n",
      "Wandb kwargs: {}\n",
      "Use data parallel: False\n",
      "\n",
      "Train data...\n",
      "    country = angola: n = 0\n",
      "    country = benin: n = 0\n",
      "    country = burkina_faso: n = 0\n",
      "    country = cameroon: n = 484\n",
      "    country = cote_d_ivoire: n = 0\n",
      "    country = democratic_republic_of_congo: n = 394\n",
      "    country = ethiopia: n = 0\n",
      "    country = ghana: n = 501\n",
      "    country = guinea: n = 0\n",
      "    country = kenya: n = 1535\n",
      "    country = lesotho: n = 658\n",
      "    country = malawi: n = 1614\n",
      "    country = mali: n = 0\n",
      "    country = mozambique: n = 741\n",
      "    country = nigeria: n = 1206\n",
      "    country = rwanda: n = 0\n",
      "    country = senegal: n = 488\n",
      "    country = sierra_leone: n = 0\n",
      "    country = tanzania: n = 0\n",
      "    country = togo: n = 274\n",
      "    country = uganda: n = 638\n",
      "    country = zambia: n = 586\n",
      "    country = zimbabwe: n = 678\n",
      "ID Val data...\n",
      "    country = angola: n = 0\n",
      "    country = benin: n = 0\n",
      "    country = burkina_faso: n = 0\n",
      "    country = cameroon: n = 45\n",
      "    country = cote_d_ivoire: n = 0\n",
      "    country = democratic_republic_of_congo: n = 49\n",
      "    country = ethiopia: n = 0\n",
      "    country = ghana: n = 59\n",
      "    country = guinea: n = 0\n",
      "    country = kenya: n = 141\n",
      "    country = lesotho: n = 66\n",
      "    country = malawi: n = 171\n",
      "    country = mali: n = 0\n",
      "    country = mozambique: n = 72\n",
      "    country = nigeria: n = 121\n",
      "    country = rwanda: n = 0\n",
      "    country = senegal: n = 47\n",
      "    country = sierra_leone: n = 0\n",
      "    country = tanzania: n = 0\n",
      "    country = togo: n = 32\n",
      "    country = uganda: n = 67\n",
      "    country = zambia: n = 63\n",
      "    country = zimbabwe: n = 67\n",
      "ID Test data...\n",
      "    country = angola: n = 0\n",
      "    country = benin: n = 0\n",
      "    country = burkina_faso: n = 0\n",
      "    country = cameroon: n = 47\n",
      "    country = cote_d_ivoire: n = 0\n",
      "    country = democratic_republic_of_congo: n = 49\n",
      "    country = ethiopia: n = 0\n",
      "    country = ghana: n = 54\n",
      "    country = guinea: n = 0\n",
      "    country = kenya: n = 154\n",
      "    country = lesotho: n = 70\n",
      "    country = malawi: n = 172\n",
      "    country = mali: n = 0\n",
      "    country = mozambique: n = 66\n",
      "    country = nigeria: n = 123\n",
      "    country = rwanda: n = 0\n",
      "    country = senegal: n = 50\n",
      "    country = sierra_leone: n = 0\n",
      "    country = tanzania: n = 0\n",
      "    country = togo: n = 24\n",
      "    country = uganda: n = 73\n",
      "    country = zambia: n = 70\n",
      "    country = zimbabwe: n = 48\n",
      "OOD Val data...\n",
      "    country = angola: n = 0\n",
      "    country = benin: n = 746\n",
      "    country = burkina_faso: n = 789\n",
      "    country = cameroon: n = 0\n",
      "    country = cote_d_ivoire: n = 0\n",
      "    country = democratic_republic_of_congo: n = 0\n",
      "    country = ethiopia: n = 0\n",
      "    country = ghana: n = 0\n",
      "    country = guinea: n = 300\n",
      "    country = kenya: n = 0\n",
      "    country = lesotho: n = 0\n",
      "    country = malawi: n = 0\n",
      "    country = mali: n = 0\n",
      "    country = mozambique: n = 0\n",
      "    country = nigeria: n = 0\n",
      "    country = rwanda: n = 0\n",
      "    country = senegal: n = 0\n",
      "    country = sierra_leone: n = 435\n",
      "    country = tanzania: n = 1639\n",
      "    country = togo: n = 0\n",
      "    country = uganda: n = 0\n",
      "    country = zambia: n = 0\n",
      "    country = zimbabwe: n = 0\n",
      "OOD Test data...\n",
      "    country = angola: n = 855\n",
      "    country = benin: n = 0\n",
      "    country = burkina_faso: n = 0\n",
      "    country = cameroon: n = 0\n",
      "    country = cote_d_ivoire: n = 341\n",
      "    country = democratic_republic_of_congo: n = 0\n",
      "    country = ethiopia: n = 1193\n",
      "    country = ghana: n = 0\n",
      "    country = guinea: n = 0\n",
      "    country = kenya: n = 0\n",
      "    country = lesotho: n = 0\n",
      "    country = malawi: n = 0\n",
      "    country = mali: n = 590\n",
      "    country = mozambique: n = 0\n",
      "    country = nigeria: n = 0\n",
      "    country = rwanda: n = 984\n",
      "    country = senegal: n = 0\n",
      "    country = sierra_leone: n = 0\n",
      "    country = tanzania: n = 0\n",
      "    country = togo: n = 0\n",
      "    country = uganda: n = 0\n",
      "    country = zambia: n = 0\n",
      "    country = zimbabwe: n = 0\n",
      "len(train_dataset)=9797\n",
      "vit_b_16 model inititalziation\n",
      "\n",
      "Epoch [0]:\n",
      "\n",
      "Train:\n",
      "len(labeled_batch)=3\n",
      "x.shape=torch.Size([32, 8, 224, 224])\n",
      "torch.Size([32, 1])\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/wilds/examples/run_expt.py\", line 479, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/wilds/examples/run_expt.py\", line 442, in main\n",
      "    train(\n",
      "  File \"/home/ubuntu/wilds/examples/train.py\", line 115, in train\n",
      "    run_epoch(algorithm, datasets['train'], general_logger, epoch, config, train=True, unlabeled_dataset=unlabeled_dataset)\n",
      "  File \"/home/ubuntu/wilds/examples/train.py\", line 50, in run_epoch\n",
      "    batch_results = algorithm.update(labeled_batch, is_epoch_end=(batch_idx==last_batch_idx))\n",
      "  File \"/home/ubuntu/wilds/examples/algorithms/single_model_algorithm.py\", line 142, in update\n",
      "    self._update(\n",
      "  File \"/home/ubuntu/wilds/examples/algorithms/single_model_algorithm.py\", line 166, in _update\n",
      "    objective.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: GET was unable to find an engine to execute this computation\n"
     ]
    }
   ],
   "source": [
    "!python examples/run_expt.py --dataset poverty --algorithm ERM --root_dir data --model vit_b_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_proj = nn.Conv2d(\n",
    "#                 in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "#             )\n",
    "\n",
    "vit = torchvision.models.vit_b_16(weights=\"DEFAULT\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(8, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.conv_proj = nn.Conv2d(\n",
    "                 in_channels=8, out_channels=768, kernel_size=(16, 16), stride=(16, 16)\n",
    "             )\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
