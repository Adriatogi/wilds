{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.2965240479\n",
      "58.1326179504\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os, time\n",
    "import torch, torchvision\n",
    "\n",
    "data_dir = '/oak/stanford/groups/akundaje/abalsubr/DREAM/wilds/codalab_archive/'\n",
    "tf = 'MAX'\n",
    "itime = time.time()\n",
    "train_chr = pd.read_csv(os.path.join(data_dir, 'labels/{}.train.labels.tsv.gz'.format(tf)), sep='\\t')\n",
    "print(time.time() - itime)\n",
    "val_chr = pd.read_csv(os.path.join(data_dir, 'labels/{}.val.labels.tsv.gz'.format(tf)), sep='\\t')\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "val_celltype = ['A549']\n",
    "test_celltype = ['GM12878']\n",
    "all_celltypes = train_celltypes + val_celltype + test_celltype\n",
    "\n",
    "metadata_map = {}\n",
    "metadata_map['chr'] = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "metadata_map['celltype'] = all_celltypes\n",
    "\n",
    "_split_dict = {\n",
    "    'train': 0,\n",
    "    'val-id': 1,\n",
    "    'test': 2,\n",
    "    'val-ood': 3\n",
    "}\n",
    "_split_names = {\n",
    "    'train': 'Train',\n",
    "    'val-id': 'Validation (ID)',\n",
    "    'test': 'Test',\n",
    "    'val-ood': 'Validation (OOD)',\n",
    "}\n",
    "_split_scheme = 'standard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.40137600899\n",
      "('chr1', 4.365410089492798)\n",
      "('chr2', 8.54686713218689)\n",
      "('chr3', 11.915641069412231)\n",
      "('chr4', 15.147382020950317)\n",
      "('chr5', 18.221237182617188)\n",
      "('chr6', 21.16081714630127)\n",
      "('chr7', 23.87936806678772)\n",
      "('chr8', 26.382845163345337)\n",
      "('chr9', 28.802964210510254)\n",
      "('chr10', 31.10539698600769)\n",
      "('chr11', 33.392733097076416)\n",
      "('chr12', 35.6597261428833)\n",
      "('chr13', 37.56297421455383)\n",
      "('chr14', 39.363978147506714)\n",
      "('chr15', 41.089357137680054)\n",
      "('chr16', 42.6117000579834)\n",
      "('chr17', 43.9806342124939)\n",
      "('chr18', 45.29493808746338)\n",
      "('chr19', 46.26894497871399)\n",
      "('chr20', 47.31300115585327)\n",
      "('chr21', 48.139018058776855)\n",
      "('chr22', 48.97876214981079)\n",
      "('chrX', 51.61549210548401)\n",
      "('H1-hESC', 24.14024806022644)\n",
      "('HCT116', 47.97159004211426)\n",
      "('HeLa-S3', 72.82926392555237)\n",
      "('HepG2', 97.18733406066895)\n",
      "('K562', 121.94148206710815)\n",
      "('A549', 147.29550194740295)\n",
      "('GM12878', 171.71312499046326)\n"
     ]
    }
   ],
   "source": [
    "itime = time.time()\n",
    "sequence_filename = os.path.join(data_dir, 'sequence.npz')\n",
    "seq_arr = np.load(sequence_filename)\n",
    "print(time.time() - itime)\n",
    "\n",
    "itime = time.time()\n",
    "_seq_bp = {}\n",
    "for chrom in seq_arr:\n",
    "    _seq_bp[chrom] = seq_arr[chrom]\n",
    "    print(chrom, time.time() - itime)\n",
    "print(\"Sequence read. Time: {}\".format(time.time() - itime))\n",
    "\n",
    "itime = time.time()\n",
    "_dnase_allcelltypes = {}\n",
    "for ct in all_celltypes:\n",
    "    dnase_filename = os.path.join(data_dir, '{}_dnase.npz'.format(ct))\n",
    "    dnase_npz_file = np.load(dnase_filename)\n",
    "    _dnase_allcelltypes[ct] = {}\n",
    "    for chrom in _seq_bp:\n",
    "        _dnase_allcelltypes[ct][chrom] = dnase_npz_file[chrom]\n",
    "    print(ct, time.time() - itime)\n",
    "print(\"DNase read for all celltypes. Time: {}\".format(time.time() - itime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-62d0e390ffe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# len(_dnase_allcelltypes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "# len(_dnase_allcelltypes)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'isin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eeaf4b928825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtr_chrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'chr2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chr9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chr11'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mte_chrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'chr1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chr8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chr21'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_chr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_chr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_chrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_chr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_chr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_chrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'isin'"
     ]
    }
   ],
   "source": [
    "tr_chrs = ['chr2', 'chr9', 'chr11']\n",
    "te_chrs = ['chr1', 'chr8', 'chr21']\n",
    "training_df = train_chr[np.isin(train_chr['chr'], tr_chrs)]\n",
    "val_df = val_chr[np.isin(val_chr['chr'], te_chrs)]\n",
    "all_df = pd.concat([training_df, val_df])\n",
    "\n",
    "#filter_msk = all_df['start'] >= 0\n",
    "filter_msk = all_df['start']%1000 == 0\n",
    "all_df = all_df[filter_msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = time.time()\n",
    "pd_list = []\n",
    "for ct in all_celltypes:\n",
    "    tc_chr = all_df[['chr', 'start', 'stop', ct]]\n",
    "    tc_chr.columns = ['chr', 'start', 'stop', 'y']\n",
    "    tc_chr['celltype'] = ct\n",
    "    pd_list.append(tc_chr)\n",
    "metadata_df = pd.concat(pd_list)\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = time.time()\n",
    "y_array = metadata_df['y'].replace({'U': 0, 'B': 1, 'A': -1}).values\n",
    "non_ambig_mask = (y_array != -1)\n",
    "metadata_df['y'] = y_array\n",
    "_metadata_df = metadata_df[non_ambig_mask]\n",
    "_y_array = torch.LongTensor(y_array[non_ambig_mask])\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itime = time.time()\n",
    "chr_ints = _metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(metadata_map['chr'])] )).values\n",
    "celltype_ints = _metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(metadata_map['celltype'])] )).values\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chr_mask = np.isin(_metadata_df['chr'], tr_chrs)\n",
    "val_chr_mask = np.isin(_metadata_df['chr'], te_chrs)\n",
    "train_celltype_mask = np.isin(_metadata_df['celltype'], train_celltypes)\n",
    "val_celltype_mask = np.isin(_metadata_df['celltype'], val_celltype)\n",
    "test_celltype_mask = np.isin(_metadata_df['celltype'], test_celltype)\n",
    "\n",
    "split_array = -1*np.ones(_metadata_df.shape[0]).astype(int)\n",
    "split_array[np.logical_and(train_chr_mask, train_celltype_mask)] = _split_dict['train']\n",
    "split_array[np.logical_and(val_chr_mask, test_celltype_mask)] = _split_dict['test']\n",
    "split_array[np.logical_and(val_chr_mask, val_celltype_mask)] = _split_dict['val-ood']\n",
    "split_array[np.logical_and(val_chr_mask, train_celltype_mask)] = _split_dict['val-id']\n",
    "_metadata_df['split'] = split_array\n",
    "_split_array = split_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-077a30fb3eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named data"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import dataset_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "class ParseKwargs(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        setattr(namespace, self.dest, dict())\n",
    "        for value in values:\n",
    "            key, value_str = value.split('=')\n",
    "            if value_str.replace('-','').isnumeric():\n",
    "                processed_val = int(value_str)\n",
    "            elif value_str.replace('-','').replace('.','').isnumeric():\n",
    "                processed_val = float(value_str)\n",
    "            elif value_str in ['True', 'true']:\n",
    "                processed_val = True\n",
    "            elif value_str in ['False', 'false']:\n",
    "                processed_val = False\n",
    "            else:\n",
    "                processed_val = value_str\n",
    "            getattr(namespace, self.dest)[key] = processed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algorithm_constructors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5b71c7aabf01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Algorithm and objective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--algorithm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm_constructors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--algorithm_kwargs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParseKwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--groupby_fields'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algorithm_constructors' is not defined"
     ]
    }
   ],
   "source": [
    "ROOTDIR = '/oak/stanford/groups/akundaje/abalsubr/wilds_other'\n",
    "args_kw = \"-d camelyon17 --algorithm ERM --model densenet121 --split_scheme standard --groupby_fields hospital --loss_function cross_entropy --optimizer SGD --lr 0.0001 --batch_size 32 --weight_decay 0 --n_epochs 10 --scheduler ReduceLROnPlateau --scheduler_metric_split val --scheduler_metric_name acc_avg --log_dir log --log_every 50 --save_step 1000 --save_best --save_last --seed 0 --evaluate_all_splits --root_dir {}\".format(\n",
    "    ROOTDIR).split()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('-d', '--dataset', choices=['encodeTFBS', 'amazon', 'camelyon17', 'celebA', 'civilcomments', 'iwildcam', 'waterbirds', 'yelp', 'poverty', 'fmow', 'ogbg-molpcba'], required=True)\n",
    "parser.add_argument('--split_scheme', default='standard',\n",
    "                    help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--root_dir', default=None, required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "parser.add_argument('--download', default=False, action='store_true',\n",
    "                    help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'], default='standard')\n",
    "parser.add_argument('--train_loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--eval_loader', choices=['standard', 'group'], default='standard')\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--no_pin_memory', action='store_true') # TODO: put as loader_kwargs\n",
    "parser.add_argument('--num_workers', type=int, default=4) # TODO: put as loader kwargs\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    '--model',\n",
    "    choices=['bert-base-uncased', 'inception_v3', 'densenet121', 'wideresnet50', 'resnet50', 'gin-virtual', 'resnet18_ms'],\n",
    "    default='resnet50')\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--train_from_scratch', action='store_true', default=False)\n",
    "\n",
    "# Algorithm and objective\n",
    "parser.add_argument('--algorithm', required=True, choices=algorithm_constructors.keys())\n",
    "parser.add_argument('--algorithm_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--groupby_fields', nargs='+', default=None)\n",
    "parser.add_argument('--loss_function', required=True, choices = losses.keys()) #TODO: make default\n",
    "parser.add_argument('--val_metric', default=None)\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int, default=4)\n",
    "parser.add_argument('--optimizer', default=None, choices=optimizer_attributes.keys())\n",
    "parser.add_argument('--lr', type=float, required=True)\n",
    "parser.add_argument('--weight_decay', type=float, required=True)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler', default=None, choices=scheduler_attributes.keys())\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--evaluate_all_splits', action='store_true', default=False)\n",
    "parser.add_argument('--additional_eval_splits', nargs='+', default=[])\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int, default=None)\n",
    "parser.add_argument('--save_best', action='store_true', default=False)\n",
    "parser.add_argument('--save_last', action='store_true', default=False)\n",
    "parser.add_argument('--save_outputs', action='store_true', default=False)\n",
    "parser.add_argument('--no_group_logging', action='store_true', default=False)\n",
    "parser.add_argument('--val_metric_decreasing', action='store_true', default=False)\n",
    "parser.add_argument('--use_wandb', action='store_true', default=False)\n",
    "parser.add_argument('--progress_bar', action='store_true', default=False)\n",
    "parser.add_argument('--resume', default=False, action='store_true')\n",
    "parser.add_argument('--eval_only', default=False, action='store_true')\n",
    "\n",
    "args = parser.parse_args(args_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_input (idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_metadata_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8b9950f817a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthis_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_metadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mitime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mflank_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_metadata_df' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "this_metadata = _metadata_df.iloc[idx, :]\n",
    "\n",
    "itime = time.time()\n",
    "flank_size = 400\n",
    "interval_start = this_metadata['start'] - flank_size\n",
    "interval_end = this_metadata['stop'] + flank_size\n",
    "dnase_this = _dnase_allcelltypes[this_metadata['celltype']][this_metadata['chr']][interval_start:interval_end]\n",
    "seq_this = _seq_bp[this_metadata['chr']][interval_start:interval_end]\n",
    "data = np.column_stack([seq_this, dnase_this])\n",
    "# print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028102874755859375\n"
     ]
    }
   ],
   "source": [
    "itime = time.time()\n",
    "metadata_array = torch.stack(\n",
    "    (torch.LongTensor(chr_ints), \n",
    "     torch.LongTensor(celltype_ints), \n",
    "     _y_array),\n",
    "    dim=1)\n",
    "print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7663873ec5ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#data.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_eval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dr_benchmark/wilds/common/data_loaders.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeightedRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_into_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dr_benchmark/wilds/common/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_scatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4600"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "interval_end\n",
    "# itime = time.time()\n",
    "# np.save(os.path.join(data_dir, 'stmp.npy'), sa)\n",
    "# print(time.time() - itime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 examples/run_expt.py -d encodeTFBS --algorithm ERM --model densenet121 --split_scheme standard --groupby_fields hospital --loss_function cross_entropy --optimizer SGD --lr 0.0001 --batch_size 32 --weight_decay 0 --n_epochs 10 --scheduler ReduceLROnPlateau --scheduler_metric_split val --scheduler_metric_name acc_avg --log_dir log --log_every 50 --save_step 1000 --save_best --save_last --seed 0 --evaluate_all_splits --root_dir ROOTDIR'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmdstr = \"python3 examples/run_expt.py -d encodeTFBS --algorithm ERM --model densenet121 --split_scheme standard --groupby_fields hospital --loss_function cross_entropy\"\n",
    "cmdstr += \" \"\n",
    "cmdstr += \"--optimizer SGD --lr 0.0001 --batch_size 32 --weight_decay 0 --n_epochs 10 --scheduler ReduceLROnPlateau --scheduler_metric_split val --scheduler_metric_name acc_avg\"\n",
    "cmdstr += \" \"\n",
    "cmdstr += \"--log_dir log --log_every 50 --save_step 1000 --save_best --save_last --seed 0 --evaluate_all_splits --root_dir ROOTDIR\"\n",
    "cmdstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_metadata_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-6a2dab26072a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_metadata_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_metadata_array' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-caa51967aeb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_eval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinatorialGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dr_benchmark/wilds/common/data_loaders.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeightedRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_into_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dr_benchmark/wilds/common/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_scatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import os, csv\n",
    "import time\n",
    "import argparse\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# TODO: Replace this once we make wilds into an installed package\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.utils import get_counts\n",
    "\n",
    "from models.model_attributes import model_attributes\n",
    "from utils import set_seed, Logger, BatchLogger, log_args, ParseKwargs, load\n",
    "from train import train, evaluate\n",
    "from data import dataset_attributes\n",
    "from optimizer import optimizer_attributes\n",
    "from scheduler import scheduler_attributes\n",
    "from loss import losses\n",
    "from utils import log_group_data\n",
    "from algorithms.constructors import algorithm_constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models.model_attributes import model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fc6a4fda67dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_attributes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVBatchLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParseKwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dr_benchmark/examples/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "def initialize_algorithm(args, datasets, train_grouper):\n",
    "    train_dataset = datasets['train']['dataset']\n",
    "    train_loader = datasets['train']['loader']\n",
    "\n",
    "    # Configure the final layer of the networks used\n",
    "    # The code below are defaults. Edit this if you need special config for your model.\n",
    "    if (train_dataset.is_classification) and (train_dataset.y_size == 1):\n",
    "        # For single-task classification, we have one output per class\n",
    "        d_out = train_dataset.n_classes\n",
    "    elif (train_dataset.is_classification) and (train_dataset.y_size > 1) and (train_dataset.n_classes == 2):\n",
    "        # For multi-task binary classification (each output is the logit for each binary class)\n",
    "        d_out = train_dataset.y_size\n",
    "    elif (not train_dataset.is_classification):\n",
    "        # For regression, we have one output per target dimension\n",
    "        d_out = train_dataset.y_size\n",
    "    else:\n",
    "        raise RuntimeError('d_out not defined.')\n",
    "        \n",
    "\n",
    "    # Sanity checking input args\n",
    "    if args.algorithm == 'groupDRO':\n",
    "        assert args.train_loader_kwargs['uniform_over_groups']\n",
    "    elif args.algorithm in ['deepCORAL', 'IRM']:\n",
    "        assert args.train_loader == 'group'\n",
    "        assert args.train_loader_kwargs['uniform_over_groups']\n",
    "        assert args.train_loader_kwargs['distinct_groups']\n",
    "\n",
    "    # Other config\n",
    "    n_train_steps = len(train_loader) * args.n_epochs\n",
    "#    prediction_fn = dataset_attributes[args.dataset]['prediction_fn']\n",
    "    loss = losses[args.loss_function]\n",
    "    metric = dataset_attributes[args.dataset]['metric']\n",
    "    train_g = train_grouper.metadata_to_group(train_dataset.metadata_array)\n",
    "    is_group_in_train = get_counts(train_g, train_grouper.n_groups) > 0\n",
    "    algorithm_constructor = algorithm_constructors[args.algorithm]\n",
    "    algorithm = algorithm_constructor(\n",
    "        args=args,\n",
    "        d_out=d_out,\n",
    "        grouper=train_grouper,\n",
    "        loss=loss,\n",
    "        metric=metric,\n",
    "        n_train_steps=n_train_steps,\n",
    "        is_group_in_train=is_group_in_train)\n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset\n",
    "    parser.add_argument('-d', '--dataset', choices=dataset_attributes.keys(), required=True)\n",
    "    parser.add_argument('--split_scheme', default='standard',\n",
    "                        help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "    parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "    parser.add_argument('--root_dir', default=None, required=True,\n",
    "                        help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "    parser.add_argument('--download', default=False, action='store_true',\n",
    "                        help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "    parser.add_argument('--frac', type=float, default=1.0,\n",
    "                        help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "    # Loaders\n",
    "    parser.add_argument('--train_loader', choices=['standard', 'group'], default='standard')\n",
    "    parser.add_argument('--train_loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "    parser.add_argument('--eval_loader', choices=['standard', 'group'], default='standard')\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--no_pin_memory', action='store_true') # TODO: put as loader_kwargs\n",
    "    parser.add_argument('--num_workers', type=int, default=4) # TODO: put as loader kwargs\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        choices=model_attributes.keys(),\n",
    "        default='resnet50')\n",
    "    parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "        help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "    parser.add_argument('--train_from_scratch', action='store_true', default=False)\n",
    "\n",
    "    # Algorithm and objective\n",
    "    parser.add_argument('--algorithm', required=True, choices=algorithm_constructors.keys())\n",
    "    parser.add_argument('--algorithm_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "    parser.add_argument('--groupby_fields', nargs='+', default=None)\n",
    "    parser.add_argument('--loss_function', required=True, choices = losses.keys()) #TODO: make default\n",
    "    parser.add_argument('--val_metric', default=None)\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument('--n_epochs', type=int, default=4)\n",
    "    parser.add_argument('--optimizer', default=None, choices=optimizer_attributes.keys())\n",
    "    parser.add_argument('--lr', type=float, required=True)\n",
    "    parser.add_argument('--weight_decay', type=float, required=True)\n",
    "    parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "    parser.add_argument('--scheduler', default=None, choices=scheduler_attributes.keys())\n",
    "    parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "    parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "    parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "    # Evaluation\n",
    "    parser.add_argument('--evaluate_all_splits', action='store_true', default=False)\n",
    "    parser.add_argument('--additional_eval_splits', nargs='+', default=[])\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--log_dir', default='./logs')\n",
    "    parser.add_argument('--log_every', default=50, type=int)\n",
    "    parser.add_argument('--save_step', type=int, default=None)\n",
    "    parser.add_argument('--save_best', action='store_true', default=False)\n",
    "    parser.add_argument('--save_last', action='store_true', default=False)\n",
    "    parser.add_argument('--save_outputs', action='store_true', default=False)\n",
    "    parser.add_argument('--no_group_logging', action='store_true', default=False)\n",
    "    parser.add_argument('--val_metric_decreasing', action='store_true', default=False)\n",
    "    parser.add_argument('--use_wandb', action='store_true', default=False)\n",
    "    parser.add_argument('--progress_bar', action='store_true', default=False)\n",
    "    parser.add_argument('--resume', default=False, action='store_true')\n",
    "    parser.add_argument('--eval_only', default=False, action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set device\n",
    "    args.device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Set defaults\n",
    "    if args.groupby_fields is None:\n",
    "        args.no_group_logging = True\n",
    "    if args.val_metric is None:\n",
    "        args.val_metric = dataset_attributes[args.dataset]['val_metric']\n",
    "\n",
    "    ## Initialize logs\n",
    "    if os.path.exists(args.log_dir) and args.resume:\n",
    "        resume=True\n",
    "        mode='a'\n",
    "    else:\n",
    "        resume=False\n",
    "        mode='w'\n",
    "    if not os.path.exists(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "    logger = Logger(os.path.join(args.log_dir, 'log.txt'), mode)\n",
    "\n",
    "    # Record args\n",
    "    log_args(args, logger)\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Data\n",
    "    full_dataset = dataset_attributes[args.dataset]['constructor'](\n",
    "        root_dir=args.root_dir,\n",
    "        download=args.download,\n",
    "        split_scheme=args.split_scheme,\n",
    "        **args.dataset_kwargs)\n",
    "\n",
    "    # To implement data augmentation (i.e., have different transforms\n",
    "    # at training time vs. test time), modify these two lines:\n",
    "    train_transform = dataset_attributes[args.dataset]['transform'](args.model)\n",
    "    if dataset_attributes[args.dataset].get('eval_transform') is None:\n",
    "        eval_transform = dataset_attributes[args.dataset]['transform'](args.model)\n",
    "    else:\n",
    "        eval_transform = dataset_attributes[args.dataset]['eval_transform'](args.model)\n",
    "\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=args.groupby_fields)\n",
    "\n",
    "    datasets = defaultdict(dict)\n",
    "    for split in full_dataset.split_dict.keys():\n",
    "        if split=='train':\n",
    "            transform = train_transform\n",
    "            verbose = True\n",
    "        elif split == 'val':\n",
    "            transform = eval_transform\n",
    "            verbose = True\n",
    "        else:\n",
    "            transform = eval_transform\n",
    "            verbose = False\n",
    "        # Get subset\n",
    "        datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "            split,\n",
    "            frac=args.frac,\n",
    "            transform=transform)\n",
    "\n",
    "        # Get loader\n",
    "        shared_loader_kwargs = {\n",
    "            'num_workers': args.num_workers,\n",
    "            'pin_memory': not args.no_pin_memory,\n",
    "            'batch_size': args.batch_size,\n",
    "            'collate_fn': dataset_attributes[args.dataset]['collate']\n",
    "        }\n",
    "\n",
    "        if split == 'train':\n",
    "            datasets[split]['loader'] = get_train_loader(\n",
    "                loader=args.train_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                grouper=train_grouper,\n",
    "                train_loader_kwargs=args.train_loader_kwargs,\n",
    "                **shared_loader_kwargs)\n",
    "        else:\n",
    "            datasets[split]['loader'] = get_eval_loader(\n",
    "                loader=args.eval_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                grouper=train_grouper,\n",
    "                **shared_loader_kwargs)\n",
    "\n",
    "        # Set fields\n",
    "        datasets[split]['split'] = split\n",
    "        datasets[split]['name'] = full_dataset.split_names[split]\n",
    "        datasets[split]['verbose'] = verbose\n",
    "        # Loggers\n",
    "        # Loggers\n",
    "        datasets[split]['eval_logger'] = BatchLogger(\n",
    "            os.path.join(args.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=args.use_wandb)\n",
    "        datasets[split]['algo_logger'] = BatchLogger(\n",
    "            os.path.join(args.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=args.use_wandb)\n",
    "\n",
    "        if args.use_wandb:\n",
    "            initialize_wandb(args)\n",
    "\n",
    "    # Logging dataset info\n",
    "    if args.no_group_logging and full_dataset.is_classification and full_dataset.y_size==1:\n",
    "        log_grouper = CombinatorialGrouper(\n",
    "            dataset=full_dataset,\n",
    "            groupby_fields=['y'])\n",
    "    elif args.no_group_logging:\n",
    "        log_grouper = None\n",
    "    else:\n",
    "        log_grouper = train_grouper\n",
    "    log_group_data(args, datasets, log_grouper, logger)\n",
    "\n",
    "    ## Initialize algorithm\n",
    "    algorithm = initialize_algorithm(args, datasets, train_grouper)\n",
    "\n",
    "    if not args.eval_only:\n",
    "        ## Load saved results if resuming\n",
    "        resume_success = False\n",
    "        if resume:\n",
    "            save_path = os.path.join(args.log_dir, 'last_model.pth')\n",
    "            if not os.path.exists(save_path):\n",
    "                epochs = [\n",
    "                    int(file.split('_')[0])\n",
    "                    for file in os.listdir(args.log_dir) if file.endswith('.pth')]\n",
    "                if len(epochs) > 0:\n",
    "                    latest_epoch = max(epochs)\n",
    "                    save_path = os.path.join(args.log_dir, f'{latest_epoch}_model.pth')\n",
    "            try:\n",
    "                prev_epoch, best_val_metric = load(algorithm, save_path)\n",
    "                epoch_offset = prev_epoch + 1\n",
    "                logger.write(f'Resuming from epoch {epoch_offset} with best val metric {best_val_metric}')\n",
    "                resume_success = True\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        if resume_success == False:\n",
    "            epoch_offset=0\n",
    "            best_val_metric=None\n",
    "\n",
    "\n",
    "        train(algorithm,\n",
    "              datasets,\n",
    "              logger,\n",
    "              args,\n",
    "              epoch_offset=epoch_offset,\n",
    "              best_val_metric=best_val_metric)\n",
    "    else:\n",
    "        best_model_path = os.path.join(args.log_dir, 'best_model.pth')\n",
    "        best_epoch, best_val_metric = load(algorithm, best_model_path)\n",
    "        evaluate(algorithm, datasets, best_epoch, logger)\n",
    "\n",
    "    logger.close()\n",
    "    for split in datasets:\n",
    "        datasets[split]['eval_logger'].close()\n",
    "        datasets[split]['algo_logger'].close()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
